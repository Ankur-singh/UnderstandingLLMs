[
  {
    "objectID": "notebooks/LLM_from_Scratch_2.html",
    "href": "notebooks/LLM_from_Scratch_2.html",
    "title": "LLM From Scratch: Part 2",
    "section": "",
    "text": "In this notebook, we incorporate architectural improvements to the LLM from the previous part. While code changes are minor, understanding the rationale is crucial. We‚Äôll discuss these changes and provide helpful resources:\n\nRMSNorm (instead of LayerNorm)\nPre-LN and Post-LN\nGated FFN - SwiGLU\nRoPE\nMinor Attention block changes will be discussed in a separate notebook for comparison (Sliding Window, Multimodal, MoE, etc.).\n\n\nSetup\nThe model architecture is copied directly from Part 1.\n\n!pip install -Uq torch\n!pip install -Uq datasets tiktoken\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Misc\nimport math\nimport tiktoken\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom dataclasses import dataclass\nfrom prettytable import PrettyTable\n\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.context = context\n        self.mha = nn.MultiheadAttention(emb_dim, heads, batch_first=True, bias=False)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n        seq_len = min(seq_len, self.context)\n        attn_mask = self.mask[:seq_len, :seq_len]\n        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n        return attn_out\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, 4 * emb_dim), nn.GELU(), nn.Linear(4 * emb_dim, emb_dim)\n        )\n        self.mha_norm = nn.LayerNorm(emb_dim)\n        self.mlp_norm = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(config.context, config.emb_dim)\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.LayerNorm(config.emb_dim)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        pos = torch.arange(seq_len, device=x.device)\n        x = self.tok_embedding(x) + self.pos_embedding(pos)\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\n# Utility Function: Number of Trainable Parameters\ndef count_parameters(model, verbose=False):\n    if verbose:\n        table = PrettyTable([\"Module\", \"Parameters\"])\n        total = 0\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                count = param.numel()\n                table.add_row([name, count])\n                total += count\n        print(table)\n    else:\n        total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Params: {total / 1e6:.2f} M\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n@dataclass\nclass ModelConfig:\n    # GPT2 architecture\n    vocab: int = math.ceil(50_257 / 64) * 64  # nearest multiple of 64\n    emb_dim: int = 768\n    heads: int = 12\n    layers: int = 12\n    context: int = 1024\n\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\ncount_parameters(model)\n\n\n\n1. RMSNorm\nThis is a simple change: replace nn.LayerNorm with nn.RMSNorm, which applies Root Mean Square Layer Normalization over a mini-batch of inputs.\n\nLearn more from the PyTorch RMSNorm docs.\nFor deeper insights and empirical results, see the RMSNorm paper.\n\n\n\n2. Post / Pre Normalization\nThe original Transformer (from the ‚ÄúAttention Is All You Need‚Äù paper) placed normalization layers after the attention and feedforward modules‚Äîthis is known as Post-Norm.\nGPT and most later LLMs use Pre-Norm, placing normalization layers before these modules.\nIn 2020, Xiong et al. showed that Pre-Norm leads to more stable gradients at initialization and can perform well without careful learning rate warm-up‚Äîunlike Post-Norm.\nFrom an implementation standpoint, the change is minor: just swap the order of operations.\n\nNote: We‚Äôre already using Pre-Norm here, so no code changes are needed.\n\n\n# Post Norm\nclass BlockPostNorm(Block):\n    def forward(self, x):\n        x = x + self.mha_norm(self.mha(x))\n        x = x + self.mlp_norm(self.nlp(x))\n\n\n# Pre Norm\nclass BlockPostNorm(Block):\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.nlp_norm(x))\n\n\n\n3. SwiGLU\nAgain, this is a small change to the FFN in the transformer block. Instead of a two-layer feedforward network with GeLU activation, we use SwiGLU.\nIt‚Äôs a short and easy-to-read paper: GLU Variants Improve Transformer\n\nclass GatedFFN(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.w1 = nn.Linear(emb_dim, 4 * int(2 / 3 * emb_dim), bias=False)\n        self.w3 = nn.Linear(emb_dim, 4 * int(2 / 3 * emb_dim), bias=False)\n        self.w2 = nn.Linear(4 * int(2 / 3 * emb_dim), emb_dim, bias=False)\n        self.silu_act = nn.SiLU()\n\n    def forward(self, x):\n        x = self.silu_act(self.w1(x)) * self.w3(x)\n        return self.w2(x)\n\nHere‚Äôs an excerpt from the paper explaining the rationale behind the 2/3 scaling factor. However, you can choose to ignore it‚Äîit was used mainly for apples-to-apples comparison in the paper.\n\n\n\nimage.png\n\n\nAt this point, the updated Transformer block (aka Block class) looks like this:\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = GatedFFN(emb_dim)  # &lt;--- update\n        self.mha_norm = nn.RMSNorm(emb_dim)  # &lt;--- update\n        self.mlp_norm = nn.RMSNorm(emb_dim)  # &lt;--- update\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\n\n4. RoPE\nIn the basic implementation, we use absolute position embeddings, as introduced in the original paper. However, things have evolved, and the current norm is to use relative position embeddings. Among various approaches, RoPE has become the dominant choice.\nIncorporating RoPE requires more than just a few line changes. It might seem overwhelming, but it‚Äôs quite simple once you understand it. We essentially apply a transformation to our Q and K vectors before the attention operation.\nLet‚Äôs first define the transformation.\n\ndef compute_freqs(dim, context_length, base=10_000):\n    assert dim % 2 == 0, \"Embedding dimension should be even\"\n    inv_freq = 1.0 / (\n        base ** (torch.arange(0, dim, 2).float() / dim)\n    )  # shape: (1, dim//2)\n    pos_ids = torch.arange(context_length)  # shape: (context_len)\n    thetas = pos_ids.unsqueeze(1) * inv_freq  # shape: (context_len, dim//2)\n    thetas = torch.cat([thetas, thetas], dim=1)  # shape: (context_len, dim)\n    return thetas.cos(), thetas.sin()\n\n\ndef apply_rope(x, cos, sin):\n    batch_size, heads, seq_len, emb_dim = x.shape\n    x1, x2 = x[..., : emb_dim // 2], x[..., emb_dim // 2 :]\n    rotated = torch.cat([-x2, x1], dim=-1)\n    cos, sin = cos[:seq_len, :], sin[:seq_len, :]\n    x_rotated = (x * cos) + (rotated * sin)\n    return x_rotated.to(dtype=x.dtype)\n\nIf the above code looks cryptic, please refer to my detailed notebook on RoPE. It implements RoPE in gradual steps to build better intuition, includes visuals, and compares different implementations.\nNote: Here, we follow the HuggingFace implementation of RoPE, as I plan to load the model weights from HF.\nAn important point about RoPE: unlike positional embeddings added once at the beginning, RoPE is applied in every transformer block.\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.heads = heads\n        self.head_dim = emb_dim // heads\n        self.qkv_proj = nn.Linear(emb_dim, 3 * emb_dim, bias=False)\n        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n        # RoPE and Casual Mask\n        cos, sin = compute_freqs(self.head_dim, context)\n        self.register_buffer(\"cos\", cos)\n        self.register_buffer(\"sin\", sin)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, emb_dim = x.shape\n        qkv = self.qkv_proj(x)\n        qkv = qkv.view(batch, seq_len, 3, self.heads, self.head_dim)\n        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, dim)\n        q, k = apply_rope(q, self.cos, self.sin), apply_rope(k, self.cos, self.sin)\n        attn = (q @ k.mT) / (self.head_dim**0.5)\n        mask = self.mask[:seq_len, :seq_len]\n        attn = attn.masked_fill(mask, float(\"-inf\"))\n        attn_out = torch.softmax(attn, dim=-1) @ v\n        attn_out = attn_out.transpose(1, 2)\n        attn_out = attn_out.reshape(batch, seq_len, -1)\n        return self.out_proj(attn_out)\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = GatedFFN(emb_dim)\n        self.mha_norm = nn.RMSNorm(emb_dim)\n        self.mlp_norm = nn.RMSNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # self.pos_embedding = nn.Embedding(config.context, config.emb_dim) # &lt;--- update\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.RMSNorm(config.emb_dim)  # &lt;--- update\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        # pos = torch.arange(seq_len, device=x.device)         # &lt;--- update\n        x = self.tok_embedding(x)  # + self.pos_embedding(pos) # &lt;--- update\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\ncount_parameters(model)\n\nWe have about 0.85 million fewer parameters, mainly by removing pos_embedding. We also eliminated bias in GatedFFN, and unlike LayerNorm, RMSNorm has no bias.\nWith these changes, let‚Äôs train the model and see if we get better scores and, consequently, better generations.\n\n\nTraining\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\nval_ds = \"\\n\\n\".join(dataset[\"test\"][\"text\"])\ntrain_ds = \"\\n\\n\".join(dataset[\"train\"][\"text\"])\n\nval_tokens = tokenizer.encode(val_ds)\ntrain_tokens = tokenizer.encode(train_ds)\nlen(val_tokens), len(train_tokens)\n\n\nclass WikiTextDataset(Dataset):\n    def __init__(self, tokens, max_len):\n        self.tokens = tokens\n        self.max_len = max_len\n\n    def __getitem__(self, idx):\n        idx = idx * self.max_len\n        x = self.tokens[idx : idx + self.max_len]\n        y = self.tokens[idx + 1 : idx + 1 + self.max_len]\n        if len(x) &lt; self.max_len:\n            x = x + [tokenizer.eot_token] * (self.max_len - len(x))\n        if len(y) &lt; self.max_len:\n            y = y + [tokenizer.eot_token] * (self.max_len - len(y))\n        return (torch.tensor(x), torch.tensor(y))\n\n    def __len__(self):\n        return math.ceil(len(self.tokens) / self.max_len)\n\n\nbatch_size = 4\nval_ds = WikiTextDataset(val_tokens, ModelConfig.context)\ntrain_ds = WikiTextDataset(train_tokens, ModelConfig.context)\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits / temp, dim=-1)  # &lt;-- update: scale using temp\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nmodel.to(device)\nevaluate(model, val_dl)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\n\n\nGradient Accumulation\nThe generations are still not very good. Generally, larger batches help when training LLMs, but we‚Äôre limited by memory here.\nGradient Accumulation is a simple way to effectively increase the batch size.\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\naccumulate = 4\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n        loss /= accumulate  # &lt;--- Update\n\n        # Backward pass\n        loss.backward()\n        if (i + 1) % accumulate == 0:  # &lt;--- Update\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\n\n\nAutomatic Mixed Precision (AMP)\nAMP is another technique to speed up training by using mixed precision instead of float32. It doesn‚Äôt reduce memory much but can speed up training. See the paper.\nIn PyTorch, it‚Äôs easy to implement: wrap the forward pass in torch.autocast(). For the backward pass, scale the loss with scaler.scale(loss).backward() and use scaler.step(optimizer) instead of optimizer.step().\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device):  # &lt;--- Update\n            logits = model(x)\n            loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\naccumulate = 4\nscaler = torch.GradScaler()\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device):  # &lt;--- Update\n            logits = model(x)\n            loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n        loss /= accumulate\n\n        # Backward pass\n        scaler.scale(loss).backward()  # &lt;--- Update\n        if (i + 1) % accumulate == 0:\n            scaler.step(optimizer)  # &lt;--- Update\n            scaler.update()  # &lt;--- Update\n            optimizer.zero_grad(set_to_none=True)\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\nAnother improvement we could add is a learning rate scheduler, but I‚Äôll leave that for now. It‚Äôs also time to add WandB logging to track our training progress."
  },
  {
    "objectID": "notebooks/Rotary_Position_Embedding.html#rotary-position-embedding-rope",
    "href": "notebooks/Rotary_Position_Embedding.html#rotary-position-embedding-rope",
    "title": "Rotary Position Embedding (RoPE)",
    "section": "Rotary Position Embedding (RoPE)",
    "text": "Rotary Position Embedding (RoPE)\nAfter listening to Umar Jamil‚Äôs talk on GPU Mode, I was really inspired. I think it was one of the best talks debunking his mindset behind learning and producing super high-quality content. I decided to apply his approach to learning and gaining a deeper understanding of various LLM components.\nIn another part of my life, I was working on a PR to add LongRoPE support for Phi3-mini-128k in torchtune. I referred to multiple LongRoPE implementations‚Äîboth within torchtune and in external projects‚Äîbut I just couldn‚Äôt get it to work.\nSo, after the talk, I thought: Huh ü§î, let me start with RoPE. I‚Äôll learn it the hard way‚Äîgoing through the paper, implementing it from scratch, then studying other people‚Äôs implementations. Only after fully understanding it (i.e., being able to implement all of it from scratch) will I say that I truly know and understand RoPE. From there, I‚Äôll build up to LongRoPE.\nThis notebook serves as a logbook, documenting everything I learn on this adventurous journey to understanding RoPE from the ground up."
  },
  {
    "objectID": "notebooks/Rotary_Position_Embedding.html#reading-rope-paper-aka-roformer",
    "href": "notebooks/Rotary_Position_Embedding.html#reading-rope-paper-aka-roformer",
    "title": "Rotary Position Embedding (RoPE)",
    "section": "Reading RoPE paper (aka RoFormer)",
    "text": "Reading RoPE paper (aka RoFormer)\nFollowing the technique outlined by Umar, I started by reading the Abstract, Introduction, and Conclusion, highlighting key points. This is a great way to gain a high-level understanding and prepares you for what to expect from the paper. Here are some important highlights from these sections:\n\nRoPE encodes absolute position using a rotation matrix while incorporating explicit relative position dependency in the self-attention formulation.\n\nRoPE has several valuable and desirable properties, including:\n\nSequence length flexibility\n\nDecaying inter-token dependency with increasing relative distances\n\nThe capability to equip linear self-attention (see LinFormer) with relative position encoding\n\n\nThe key idea is to encode relative position by multiplying context representations with a rotation matrix.\n\nWith absolute position information encoded through a rotation matrix, relative position can be naturally formulated using vector projection in self-attention.\n\nFor the math part, it‚Äôs highly recommended to use pen and paper‚Äîwrite down the equations and work through the derivations. TBH, I still don‚Äôt fully understand all the mathematics in the RoFormer paper. But the process of writing things down forces your brain to pick up details that it wouldn‚Äôt through passive reading.\nHere are a few minor things I picked up:\n\nAbsolute Position Embeddings, whether from a predefined sinusoidal function (as in the original Transformer by Vaswani et al.) or as trainable vectors, add position information to all q, k, and v vectors. Look at the subscript of function ( \\(f\\) ).\n\n\n\n\nimage.png\n\n\n\n(Almost all) Relative Position Embeddings add position information to only k and v vectors.\n\n\n\n\nimage.png\n\n\n\nFinally, RoPE adds position information to only q and k, not the v vector.\n\n\n\n\nimage.png\n\n\nAnother important difference here is that both absolute and relative methods add position encoding to the context representations. However, RoPE multiplies position encoding and context representation. This point is emphasized repeatedly in the paper, but writing it down on paper really helped solidify it for me.\nThese differences are subtle but quite important to fully understand and appreciate RoPE. After picking up on these subtle distinctions, I went through the paper again, looking for the authors‚Äô motivation for making these choices. Here is an excerpt from Section 3.1 in the paper:\n\n\n\nimage.png\n\n\n\nSentence 1 & 2: Since positional information is leveraged in the self-attention mechanism, particularly in the ( \\(q_m^T k_n\\) ) operation, the authors decided to update only the q and k vectors with position information.\n\nSentence 3: With all the relative embedding papers, the benefits of relative position encoding were clear. So, they chose to use only relative position information during the inner product of the query and key vectors.\n\nLooks like the authors are some sort of math ninjas‚Äîthey quickly provide the solution (i.e., the mathematical transformation that meets both constraints). Here is the solution for the 2D case:\n\n\n\nimage.png\n\n\nHere is the general form of the solution:\n\n\n\nimage.png\n\n\nJust after presenting the general form solution, the authors quickly point out that due to sparsity, applying matrix multiplication directly is not very computationally efficient (see the last two lines in the above image). They also present a more computationally efficient realization.\n\n\n\nimage.png\n\n\nEquipped with all this information, I was eager to implement RoPE in PyTorch. But before I jump to the next section, here are three important lessons when reading the paper:\n\nFor theoretical sections like the abstract, introduction, and conclusion, highlight key points as you read.\nFor the mathematics, use pen and paper. Highly recommended!\nRe-read the paper a few times."
  },
  {
    "objectID": "notebooks/Rotary_Position_Embedding.html#implementing-rope",
    "href": "notebooks/Rotary_Position_Embedding.html#implementing-rope",
    "title": "Rotary Position Embedding (RoPE)",
    "section": "Implementing RoPE",
    "text": "Implementing RoPE\n\nimport torch\nimport matplotlib.pyplot as plt\n\n\n# batch_size, seq_len, dim\nB, S, D = 4, 8, 128\nx = torch.randn(B, S, D)\n\n# RoPE initializations\nbase = 10_000\n\n\ndef get_theta(S, D):\n    pos_ids = torch.arange(S, dtype=torch.float)\n    freqs = base ** (torch.arange(0, D, 2) / D)\n    theta = torch.outer(pos_ids, 1.0 / freqs)\n    return theta\n\n\nprint(f\"{x.shape=}\")\ntheta = get_theta(S, D)\nprint(f\"{theta.shape=}\")\n\n\nCompute Efficient Implementation (based on Equation 34)\nBelow is a naive one-to-one implementation of the computationally efficient realization of the rotary matrix based on equation 34 from the paper. Let me jot down the steps:\n\nCalculate freqs (\\(\\theta\\)) and pos_ids (\\(m\\))\nTake the outer product to get a matrix of all \\(m\\theta\\) pairs\nDuplicate \\(\\theta\\) values and rearrange them\nCalculate \\(cos\\) and \\(sin\\)\nCalculate the transformed (\\(x\\_half\\)) vector in the second half of the equation\nFinally, calculate the rotated vector using \\(x = (x * \\cos m\\theta) + (x\\_half * \\sin m\\theta)\\)\n\n\ndef apply_rope34(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # [batch, seq_len, dim//2]\n    sin, cos = theta.sin(), theta.cos()\n\n    # Expand [t1, t2, t3, t4] -&gt; [t1, t1, t2, t2, t3, t3, t4, t4]\n    sin_pos = torch.stack([sin, sin], dim=-1).view(S, D)\n    cos_pos = torch.stack([cos, cos], dim=-1).view(S, D)\n\n    # 2nd term: [-x2, x1, -x4, x3, -x6, x5, -x8, x7]\n    x_half = torch.stack([-x[..., 1::2], x[..., 0::2]], dim=-1).reshape_as(x)\n\n    x_rot = x * cos_pos + x_half * sin_pos\n    return x_rot\n\n\nx_rotated = apply_rope34(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\nThe above code is similar to the RoFormer implementation in the transformers library.\n\n\nSimplified 2D Implementation (based on equation 13)\n\n\n\nimage.png\n\n\nIf we assume that the embedding \\(x\\) is already multiplied with the \\(Q\\) and \\(K\\) matrices, then we can simplify the above equation as follows:\n\\[\\begin{align}\n    f(x_m, m) = \\begin{pmatrix}\n    \\cos m\\theta & \\sin m\\theta \\\\\n    \\sin m\\theta & \\cos m\\theta\n    \\end{pmatrix} \\begin{pmatrix}\n    x_1 \\\\ x_2\n    \\end{pmatrix}\n    &= \\begin{pmatrix}\n    x_1 * \\cos m\\theta - x_2 * \\sin m\\theta \\\\\n    x_1 * \\sin m\\theta - x_2 * \\cos m\\theta\n    \\end{pmatrix}\n\\end{align}\\]\nHere‚Äôs what the code implementation looks like:\n\ndef apply_rope13(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # [batch, seq_len, dim//2]\n    sin, cos = theta.sin(), theta.cos()\n\n    # even and odd terms\n    x1, x2 = x[..., 0::2], x[..., 1::2]\n\n    # [cos_nŒ∏, -sin_nŒ∏] [x1]\n    # [sin_nŒ∏,  cos_nŒ∏] [x2]\n    # =&gt; [x1 * cos_nŒ∏ - x2 * sin_nŒ∏, x1 * sin_nŒ∏ + x2 * cos_nŒ∏]\n    x_rot = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1).reshape_as(\n        x\n    )\n    return x_rot\n\n\nx_rotated = apply_rope13(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\n\n# Both implementation should produce same result\nassert torch.allclose(apply_rope34(x), apply_rope13(x)), \"Not Equal\"\n\nIn this approach, we don‚Äôt have to duplicate theta. Ideally, this should save some memory. The official RoFormer PyTorch repo inspired the above implementation.\n\n\nLlama3.1 Implementation\nMost RoPE implementations I found online makes use one of the two implementations above. But RoPE implementation in Meta Llama3.1 implements RoPE directly using equation 12 with complex number mathematics. While It might sound complex, but the code is pretty concise.\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    ndim = x.ndim\n    assert 0 &lt;= 1 &lt; ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\n\ndef apply_rope_llama31(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # constructs a complex tensor from polar coordinates\n    freq_cis = torch.polar(torch.ones_like(theta), theta)\n\n    # turn `x` into complex number\n    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n    freq_cis = reshape_for_broadcast(freq_cis, x_)\n\n    # Real[X * e^(imŒ∏)]\n    x_rotated = torch.view_as_real(x_ * freq_cis).flatten(-2)\n    return x_rotated\n\n\nx_rotated = apply_rope_llama31(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\n\n# should be equal to other two implementations\nassert torch.allclose(apply_rope_llama31(x), apply_rope13(x)), \"Not Equal\"\nassert torch.allclose(apply_rope_llama31(x), apply_rope34(x)), \"Not Equal\"\n\nThe complete code can be found here: Llama3.1 RoPE implementation. Also, Umar Jamil has this amazing YouTube video where he codes Llama2 from scratch, implementing RoPE step-by-step.\nThe last objective was to check out the HF implementation and call it a day. But I didn‚Äôt know what I was stepping into.\n\n\nHuggingFace implementation\nI decided to look at the RoPE implementation in the Llama model. Their implementation was very similar to our equation 34 implementation, where we duplicated theta, calculated cos, sin, and x_half, and finally combined them to derive x_rotated.\nBut there was one subtle difference in how they calculated x_half. Here is the code snippet:\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\nInstead of taking even and odd indexes, they simply take the first and second halves. My initial thought was, ‚ÄúThis seems wrong,‚Äù but if that were the case, others would have noticed it. Maybe I‚Äôm missing something. So, I extracted the relevant parts and compared them with my implementation. The results didn‚Äôt match.\nMy next hunch was that they might have used some combination of reshape, transpose, and broadcast to do it and still get the same results. But why? My guess was performance!\nI started hunting for these magic transformations in the codebase. After wasting a couple of hours looking through the code, copy-pasting, writing code, and seeking help from LLMs (conversation link: ChatGPT & Claude), I became sure that the HF implementation was wrong (even though it‚Äôs very unlikely).\nThe next step was to create an issue, but being a good open-source contributor and to avoid duplication, I decided to first search for it. I realized that many people before me had the same realization. Here is one such issue with an in-depth discussion and numerous resources. TL;DR: comment.\nBasically, when adding any model to the Transformers library, one has to include a convert_xxxx_weights_to_hf.py script. This script converts the original weight and layer names to fit the HF convention. The common belief is that weights should not be changed. But the convert script for Llama has a premute function that rearranges the weights:\n# permute for sliced rotary\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\nThis re-arrangement is what allows them to use x[..., :D/2] and x[..., D/2:] in the RoPE operation, rather than splitting (x) by even and odd indices. Therefore, my guess was right‚Äîthey do have the magic transformation that I was looking for, but it‚Äôs not in the training/inference code, it‚Äôs in the convert script. This can be a source of confusion, as it‚Äôs not very well documented and is located in a completely different part of the codebase.\nThis discovery was a fantastic outcome that truly validated my understanding of RoPE. It was a subtle, latent finding that confirmed my thought process and added a layer of depth to my comprehension. The whole journey was a thrilling adventure filled with learning, and I thoroughly enjoyed the process. It‚Äôs a perfect example of how diving deep into a problem not only strengthens your expertise but also makes the journey of discovery incredibly fun."
  },
  {
    "objectID": "notebooks/Rotary_Position_Embedding.html#building-intuition-about-rope",
    "href": "notebooks/Rotary_Position_Embedding.html#building-intuition-about-rope",
    "title": "Rotary Position Embedding (RoPE)",
    "section": "Building intuition about RoPE",
    "text": "Building intuition about RoPE\nNote: This section was added later, on April 13th, 2025.\nHaving a good intuition about RoPE is really important for understanding various RoPE-based LLM context window extension methods. After spending few weeks going through different papers, implementing them, and pulling my hairs out for days to make of it all, I found that visualizing the theta matrix was one of the most effective ways to understand them. Visualization turned out to be a game changer in this quest.\nLet‚Äôs say your embedding vector \\((X)\\), for a token, is of shape \\([1, 16]\\). RoPE introduces a rotation vector \\((\\theta)\\) which is half the size of \\(X\\) i.e., \\([1, 8]\\). The values in \\(X\\) are rotated in pairs of two ‚Äì so \\(x_1\\) and \\(x_2\\) will be rotated by angle \\(\\theta_1\\). Similarly, other pairs will be rotated by their corresponding angles. I have color-coded the elements of \\(X\\) and \\(\\theta\\) to make it easy to visualize.\n\n\n\nimage.png\n\n\nSo, the first and obvious question is: Where do we get \\(\\theta\\)? Simple, we calculate it using the formula: \\[\\theta_d = b^{-2d/|D|}\\] Here, \\(b\\) is called base (generally set to 10,000), and \\(|D|\\) is dimensionality of \\(X\\) (which is 16 in our example).\nBelow is the pytorch code to calculate the \\(\\theta\\) vector:\n\nemb_dim = 16\nbase = 10_000\n\ntheta = 1.0 / (base ** (torch.arange(0, emb_dim, 2) / emb_dim))\ntheta\n\nLets plot our theta values\n\nplt.plot(theta)\nplt.title(\"Theta Vector\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Theta value\")\nplt.show()\n\nHere‚Äôs how to interpret the plot: - Y-axis shows the actual value of \\(Œ∏\\) (theta) for each index. - From left to right, the \\(Œ∏\\) value decay exponentially ‚Äì lower dimensions are assigned higher frequencies (large \\(Œ∏\\) values), while higher dimension receive lower frequencies. - The rotation angle becomes nearly zero for higher dimensions. In the plot above, indices 5, 6, and 7 are all approximately zero.\nBeing able to simply calculate these values is what makes RoPE flexible for extending context window. Since these values are not learnt, we can change them even after training allowing us to easily extend the context window of pre-trained LLMs that use RoPE.\n\nProcessing Sequence\nMoving on, when processing a sequence, we have multiple tokens. To encode positional information, we multiply the rotation vector \\(Œ∏\\) by the position of each token in the sequence.\nAs shown in the image below, the same rotation vector \\((\\theta)\\) is scaled (i.e.¬†multiplied) by the position of the token in the sequence. This results in a 2D matrix, where each value represents the angle by which a corresponding pair of embedding values will be rotated. Notice, each row of this matrix (i.e., each token position) contains distinct angles, enabling the model to differentiate ‚Äì and thus learn ‚Äì based on token position.\n\n\n\nimage.png\n\n\nHere is the pytorch code for generating the matrix\n\nmax_seq_len = 8\n\npos_ids = torch.arange(0, max_seq_len)\nthetas = torch.outer(pos_ids, theta)\nthetas.shape\n\nWe have a theta vector that is 8D, and we can have at most 8 tokens in the sequence. Plotting thetas matrix:\n\nfor i, t in enumerate(thetas):\n    plt.plot(t, label=f\"Position {i}\")\nplt.title(\"Theta Vector\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Theta value\")\nplt.legend()\nplt.show()\n\nAs the token position in the sequence increases, the angle of rotation also increases linearly.\nHowever, theta values for the higher dimensions (indices 5, 6, and 7) are almost indistinguishable across different token positions. This effect is much clearer when visualized using in a heatmap.\n\nplt.imshow(thetas, cmap=\"Blues\")\nplt.colorbar()\nplt.title(\"Theta Matrix\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Token Position\")\nplt.show()\n\nWow, most values in the matrix are close to zero!\nWhat you‚Äôre looking at is a 2D matrix where:\n\nGoing left to right (i.e., as the index of the theta vector increases), the values decrease.\nGoing top to bottom (i.e., as the token position increases), the values increase.\n\nThis pattern is quite important to understand. So, here‚Äôs a quick question to test your grasp of it:\nWhere is the smallest value and where is the largest value in the matrix?\nNow, scroll back up, and take a closer look at get_theta function defined above. At this point, it should be a piece of cake to both understand and even write this function from scratch!"
  },
  {
    "objectID": "notebooks/Rotary_Position_Embedding.html#further-work",
    "href": "notebooks/Rotary_Position_Embedding.html#further-work",
    "title": "Rotary Position Embedding (RoPE)",
    "section": "Further Work",
    "text": "Further Work\nNow that we understand the mechanics of RoPE embeddings, I plan to dive deeper into various methods of extending the context window in LLMs that are built on top of RoPE. Specifically, I want to better understand the how theta values are manipulated to extend context window. The goal is to develop a more comprehensive understanding of RoPE by exploring it from multiple perspectives. I believe this will provide valuable insights and make it easier to further deepen my knowledge in this area."
  },
  {
    "objectID": "nbs/LLM_from_Scratch_2.html",
    "href": "nbs/LLM_from_Scratch_2.html",
    "title": "Setup",
    "section": "",
    "text": "In this notebook, we incorporate architectural improvements to the LLM from the previous part. While code changes are minor, understanding the rationale is crucial. We‚Äôll discuss these changes and provide helpful resources:\n\nRMSNorm (instead of LayerNorm)\nPre-LN and Post-LN\nGated FFN - SwiGLU\nRoPE\nMinor Attention block changes will be discussed in a separate notebook for comparison (Sliding Window, Multimodal, MoE, etc.).\n\n\nSetup\nThe model architecture is copied directly from Part 1.\n\n!pip install -Uq torch\n!pip install -Uq datasets tiktoken\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Misc\nimport math\nimport tiktoken\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom dataclasses import dataclass\nfrom prettytable import PrettyTable\n\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.context = context\n        self.mha = nn.MultiheadAttention(emb_dim, heads, batch_first=True, bias=False)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n        seq_len = min(seq_len, self.context)\n        attn_mask = self.mask[:seq_len, :seq_len]\n        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n        return attn_out\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, 4 * emb_dim), nn.GELU(), nn.Linear(4 * emb_dim, emb_dim)\n        )\n        self.mha_norm = nn.LayerNorm(emb_dim)\n        self.mlp_norm = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(config.context, config.emb_dim)\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.LayerNorm(config.emb_dim)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        pos = torch.arange(seq_len, device=x.device)\n        x = self.tok_embedding(x) + self.pos_embedding(pos)\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\n# Utility Function: Number of Trainable Parameters\ndef count_parameters(model, verbose=False):\n    if verbose:\n        table = PrettyTable([\"Module\", \"Parameters\"])\n        total = 0\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                count = param.numel()\n                table.add_row([name, count])\n                total += count\n        print(table)\n    else:\n        total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Params: {total / 1e6:.2f} M\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n@dataclass\nclass ModelConfig:\n    # GPT2 architecture\n    vocab: int = math.ceil(50_257 / 64) * 64  # nearest multiple of 64\n    emb_dim: int = 768\n    heads: int = 12\n    layers: int = 12\n    context: int = 1024\n\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\ncount_parameters(model)\n\n\n\n1. RMSNorm\nThis is a simple change: replace nn.LayerNorm with nn.RMSNorm, which applies Root Mean Square Layer Normalization over a mini-batch of inputs.\n\nLearn more from the PyTorch RMSNorm docs.\nFor deeper insights and empirical results, see the RMSNorm paper.\n\n\n\n2. Post / Pre Normalization\nThe original Transformer (from the ‚ÄúAttention Is All You Need‚Äù paper) placed normalization layers after the attention and feedforward modules‚Äîthis is known as Post-Norm.\nGPT and most later LLMs use Pre-Norm, placing normalization layers before these modules.\nIn 2020, Xiong et al. showed that Pre-Norm leads to more stable gradients at initialization and can perform well without careful learning rate warm-up‚Äîunlike Post-Norm.\nFrom an implementation standpoint, the change is minor: just swap the order of operations.\n\nNote: We‚Äôre already using Pre-Norm here, so no code changes are needed.\n\n\n# Post Norm\nclass BlockPostNorm(Block):\n    def forward(self, x):\n        x = x + self.mha_norm(self.mha(x))\n        x = x + self.mlp_norm(self.nlp(x))\n\n\n# Pre Norm\nclass BlockPostNorm(Block):\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.nlp_norm(x))\n\n\n\n3. SwiGLU\nAgain, this is a small change to the FFN in the transformer block. Instead of a two-layer feedforward network with GeLU activation, we use SwiGLU.\nIt‚Äôs a short and easy-to-read paper: GLU Variants Improve Transformer\n\nclass GatedFFN(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.w1 = nn.Linear(emb_dim, 4 * int(2 / 3 * emb_dim), bias=False)\n        self.w3 = nn.Linear(emb_dim, 4 * int(2 / 3 * emb_dim), bias=False)\n        self.w2 = nn.Linear(4 * int(2 / 3 * emb_dim), emb_dim, bias=False)\n        self.silu_act = nn.SiLU()\n\n    def forward(self, x):\n        x = self.silu_act(self.w1(x)) * self.w3(x)\n        return self.w2(x)\n\nHere‚Äôs an excerpt from the paper explaining the rationale behind the 2/3 scaling factor. However, you can choose to ignore it‚Äîit was used mainly for apples-to-apples comparison in the paper.\n\n\n\nimage.png\n\n\nAt this point, the updated Transformer block (aka Block class) looks like this:\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = GatedFFN(emb_dim)  # &lt;--- update\n        self.mha_norm = nn.RMSNorm(emb_dim)  # &lt;--- update\n        self.mlp_norm = nn.RMSNorm(emb_dim)  # &lt;--- update\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\n\n4. RoPE\nIn the basic implementation, we use absolute position embeddings, as introduced in the original paper. However, things have evolved, and the current norm is to use relative position embeddings. Among various approaches, RoPE has become the dominant choice.\nIncorporating RoPE requires more than just a few line changes. It might seem overwhelming, but it‚Äôs quite simple once you understand it. We essentially apply a transformation to our Q and K vectors before the attention operation.\nLet‚Äôs first define the transformation.\n\ndef compute_freqs(dim, context_length, base=10_000):\n    assert dim % 2 == 0, \"Embedding dimension should be even\"\n    inv_freq = 1.0 / (\n        base ** (torch.arange(0, dim, 2).float() / dim)\n    )  # shape: (1, dim//2)\n    pos_ids = torch.arange(context_length)  # shape: (context_len)\n    thetas = pos_ids.unsqueeze(1) * inv_freq  # shape: (context_len, dim//2)\n    thetas = torch.cat([thetas, thetas], dim=1)  # shape: (context_len, dim)\n    return thetas.cos(), thetas.sin()\n\n\ndef apply_rope(x, cos, sin):\n    batch_size, heads, seq_len, emb_dim = x.shape\n    x1, x2 = x[..., : emb_dim // 2], x[..., emb_dim // 2 :]\n    rotated = torch.cat([-x2, x1], dim=-1)\n    cos, sin = cos[:seq_len, :], sin[:seq_len, :]\n    x_rotated = (x * cos) + (rotated * sin)\n    return x_rotated.to(dtype=x.dtype)\n\nIf the above code looks cryptic, please refer to my detailed notebook on RoPE. It implements RoPE in gradual steps to build better intuition, includes visuals, and compares different implementations.\nNote: Here, we follow the HuggingFace implementation of RoPE, as I plan to load the model weights from HF.\nAn important point about RoPE: unlike positional embeddings added once at the beginning, RoPE is applied in every transformer block.\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.heads = heads\n        self.head_dim = emb_dim // heads\n        self.qkv_proj = nn.Linear(emb_dim, 3 * emb_dim, bias=False)\n        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n        # RoPE and Casual Mask\n        cos, sin = compute_freqs(self.head_dim, context)\n        self.register_buffer(\"cos\", cos)\n        self.register_buffer(\"sin\", sin)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, emb_dim = x.shape\n        qkv = self.qkv_proj(x)\n        qkv = qkv.view(batch, seq_len, 3, self.heads, self.head_dim)\n        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, dim)\n        q, k = apply_rope(q, self.cos, self.sin), apply_rope(k, self.cos, self.sin)\n        attn = (q @ k.mT) / (self.head_dim**0.5)\n        mask = self.mask[:seq_len, :seq_len]\n        attn = attn.masked_fill(mask, float(\"-inf\"))\n        attn_out = torch.softmax(attn, dim=-1) @ v\n        attn_out = attn_out.transpose(1, 2)\n        attn_out = attn_out.reshape(batch, seq_len, -1)\n        return self.out_proj(attn_out)\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = GatedFFN(emb_dim)\n        self.mha_norm = nn.RMSNorm(emb_dim)\n        self.mlp_norm = nn.RMSNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.mha_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # self.pos_embedding = nn.Embedding(config.context, config.emb_dim) # &lt;--- update\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.RMSNorm(config.emb_dim)  # &lt;--- update\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        # pos = torch.arange(seq_len, device=x.device)         # &lt;--- update\n        x = self.tok_embedding(x)  # + self.pos_embedding(pos) # &lt;--- update\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\ncount_parameters(model)\n\nWe have about 0.85 million fewer parameters, mainly by removing pos_embedding. We also eliminated bias in GatedFFN, and unlike LayerNorm, RMSNorm has no bias.\nWith these changes, let‚Äôs train the model and see if we get better scores and, consequently, better generations.\n\n\nTraining\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\nval_ds = \"\\n\\n\".join(dataset[\"test\"][\"text\"])\ntrain_ds = \"\\n\\n\".join(dataset[\"train\"][\"text\"])\n\nval_tokens = tokenizer.encode(val_ds)\ntrain_tokens = tokenizer.encode(train_ds)\nlen(val_tokens), len(train_tokens)\n\n\nclass WikiTextDataset(Dataset):\n    def __init__(self, tokens, max_len):\n        self.tokens = tokens\n        self.max_len = max_len\n\n    def __getitem__(self, idx):\n        idx = idx * self.max_len\n        x = self.tokens[idx : idx + self.max_len]\n        y = self.tokens[idx + 1 : idx + 1 + self.max_len]\n        if len(x) &lt; self.max_len:\n            x = x + [tokenizer.eot_token] * (self.max_len - len(x))\n        if len(y) &lt; self.max_len:\n            y = y + [tokenizer.eot_token] * (self.max_len - len(y))\n        return (torch.tensor(x), torch.tensor(y))\n\n    def __len__(self):\n        return math.ceil(len(self.tokens) / self.max_len)\n\n\nbatch_size = 4\nval_ds = WikiTextDataset(val_tokens, ModelConfig.context)\ntrain_ds = WikiTextDataset(train_tokens, ModelConfig.context)\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits / temp, dim=-1)  # &lt;-- update: scale using temp\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nmodel.to(device)\nevaluate(model, val_dl)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\n\n\nGradient Accumulation\nThe generations are still not very good. Generally, larger batches help when training LLMs, but we‚Äôre limited by memory here.\nGradient Accumulation is a simple way to effectively increase the batch size.\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\naccumulate = 4\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n        loss /= accumulate  # &lt;--- Update\n\n        # Backward pass\n        loss.backward()\n        if (i + 1) % accumulate == 0:  # &lt;--- Update\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\n\n\nAutomatic Mixed Precision (AMP)\nAMP is another technique to speed up training by using mixed precision instead of float32. It doesn‚Äôt reduce memory much but can speed up training. See the paper.\nIn PyTorch, it‚Äôs easy to implement: wrap the forward pass in torch.autocast(). For the backward pass, scale the loss with scaler.scale(loss).backward() and use scaler.step(optimizer) instead of optimizer.step().\n\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device):  # &lt;--- Update\n            logits = model(x)\n            loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nprefix = \"Once upon a time\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nlog_freq = 40\nepochs = 2\nlosses = []\naccumulate = 4\nscaler = torch.GradScaler()\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device):  # &lt;--- Update\n            logits = model(x)\n            loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n        loss /= accumulate\n\n        # Backward pass\n        scaler.scale(loss).backward()  # &lt;--- Update\n        if (i + 1) % accumulate == 0:\n            scaler.step(optimizer)  # &lt;--- Update\n            scaler.update()  # &lt;--- Update\n            optimizer.zero_grad(set_to_none=True)\n\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"Internet is an\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"AI will\"))\nprint(\"=\" * 15)\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\nAnother improvement we could add is a learning rate scheduler, but I‚Äôll leave that for now. It‚Äôs also time to add WandB logging to track our training progress."
  },
  {
    "objectID": "nbs/Rotary_Position_Embedding.html#rotary-position-embedding-rope",
    "href": "nbs/Rotary_Position_Embedding.html#rotary-position-embedding-rope",
    "title": "Understanding LLMs",
    "section": "Rotary Position Embedding (RoPE)",
    "text": "Rotary Position Embedding (RoPE)\nAfter listening to Umar Jamil‚Äôs talk on GPU Mode, I was really inspired. I think it was one of the best talks debunking his mindset behind learning and producing super high-quality content. I decided to apply his approach to learning and gaining a deeper understanding of various LLM components.\nIn another part of my life, I was working on a PR to add LongRoPE support for Phi3-mini-128k in torchtune. I referred to multiple LongRoPE implementations‚Äîboth within torchtune and in external projects‚Äîbut I just couldn‚Äôt get it to work.\nSo, after the talk, I thought: Huh ü§î, let me start with RoPE. I‚Äôll learn it the hard way‚Äîgoing through the paper, implementing it from scratch, then studying other people‚Äôs implementations. Only after fully understanding it (i.e., being able to implement all of it from scratch) will I say that I truly know and understand RoPE. From there, I‚Äôll build up to LongRoPE.\nThis notebook serves as a logbook, documenting everything I learn on this adventurous journey to understanding RoPE from the ground up."
  },
  {
    "objectID": "nbs/Rotary_Position_Embedding.html#reading-rope-paper-aka-roformer",
    "href": "nbs/Rotary_Position_Embedding.html#reading-rope-paper-aka-roformer",
    "title": "Understanding LLMs",
    "section": "Reading RoPE paper (aka RoFormer)",
    "text": "Reading RoPE paper (aka RoFormer)\nFollowing the technique outlined by Umar, I started by reading the Abstract, Introduction, and Conclusion, highlighting key points. This is a great way to gain a high-level understanding and prepares you for what to expect from the paper. Here are some important highlights from these sections:\n\nRoPE encodes absolute position using a rotation matrix while incorporating explicit relative position dependency in the self-attention formulation.\n\nRoPE has several valuable and desirable properties, including:\n\nSequence length flexibility\n\nDecaying inter-token dependency with increasing relative distances\n\nThe capability to equip linear self-attention (see LinFormer) with relative position encoding\n\n\nThe key idea is to encode relative position by multiplying context representations with a rotation matrix.\n\nWith absolute position information encoded through a rotation matrix, relative position can be naturally formulated using vector projection in self-attention.\n\nFor the math part, it‚Äôs highly recommended to use pen and paper‚Äîwrite down the equations and work through the derivations. TBH, I still don‚Äôt fully understand all the mathematics in the RoFormer paper. But the process of writing things down forces your brain to pick up details that it wouldn‚Äôt through passive reading.\nHere are a few minor things I picked up:\n\nAbsolute Position Embeddings, whether from a predefined sinusoidal function (as in the original Transformer by Vaswani et al.) or as trainable vectors, add position information to all q, k, and v vectors. Look at the subscript of function ( \\(f\\) ).\n\n\n\n\nimage.png\n\n\n\n(Almost all) Relative Position Embeddings add position information to only k and v vectors.\n\n\n\n\nimage.png\n\n\n\nFinally, RoPE adds position information to only q and k, not the v vector.\n\n\n\n\nimage.png\n\n\nAnother important difference here is that both absolute and relative methods add position encoding to the context representations. However, RoPE multiplies position encoding and context representation. This point is emphasized repeatedly in the paper, but writing it down on paper really helped solidify it for me.\nThese differences are subtle but quite important to fully understand and appreciate RoPE. After picking up on these subtle distinctions, I went through the paper again, looking for the authors‚Äô motivation for making these choices. Here is an excerpt from Section 3.1 in the paper:\n\n\n\nimage.png\n\n\n\nSentence 1 & 2: Since positional information is leveraged in the self-attention mechanism, particularly in the ( \\(q_m^T k_n\\) ) operation, the authors decided to update only the q and k vectors with position information.\n\nSentence 3: With all the relative embedding papers, the benefits of relative position encoding were clear. So, they chose to use only relative position information during the inner product of the query and key vectors.\n\nLooks like the authors are some sort of math ninjas‚Äîthey quickly provide the solution (i.e., the mathematical transformation that meets both constraints). Here is the solution for the 2D case:\n\n\n\nimage.png\n\n\nHere is the general form of the solution:\n\n\n\nimage.png\n\n\nJust after presenting the general form solution, the authors quickly point out that due to sparsity, applying matrix multiplication directly is not very computationally efficient (see the last two lines in the above image). They also present a more computationally efficient realization.\n\n\n\nimage.png\n\n\nEquipped with all this information, I was eager to implement RoPE in PyTorch. But before I jump to the next section, here are three important lessons when reading the paper:\n\nFor theoretical sections like the abstract, introduction, and conclusion, highlight key points as you read.\nFor the mathematics, use pen and paper. Highly recommended!\nRe-read the paper a few times."
  },
  {
    "objectID": "nbs/Rotary_Position_Embedding.html#implementing-rope",
    "href": "nbs/Rotary_Position_Embedding.html#implementing-rope",
    "title": "Understanding LLMs",
    "section": "Implementing RoPE",
    "text": "Implementing RoPE\n\nimport torch\nimport matplotlib.pyplot as plt\n\n\n# batch_size, seq_len, dim\nB, S, D = 4, 8, 128\nx = torch.randn(B, S, D)\n\n# RoPE initializations\nbase = 10_000\n\n\ndef get_theta(S, D):\n    pos_ids = torch.arange(S, dtype=torch.float)\n    freqs = base ** (torch.arange(0, D, 2) / D)\n    theta = torch.outer(pos_ids, 1.0 / freqs)\n    return theta\n\n\nprint(f\"{x.shape=}\")\ntheta = get_theta(S, D)\nprint(f\"{theta.shape=}\")\n\n\nCompute Efficient Implementation (based on Equation 34)\nBelow is a naive one-to-one implementation of the computationally efficient realization of the rotary matrix based on equation 34 from the paper. Let me jot down the steps:\n\nCalculate freqs (\\(\\theta\\)) and pos_ids (\\(m\\))\nTake the outer product to get a matrix of all \\(m\\theta\\) pairs\nDuplicate \\(\\theta\\) values and rearrange them\nCalculate \\(cos\\) and \\(sin\\)\nCalculate the transformed (\\(x\\_half\\)) vector in the second half of the equation\nFinally, calculate the rotated vector using \\(x = (x * \\cos m\\theta) + (x\\_half * \\sin m\\theta)\\)\n\n\ndef apply_rope34(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # [batch, seq_len, dim//2]\n    sin, cos = theta.sin(), theta.cos()\n\n    # Expand [t1, t2, t3, t4] -&gt; [t1, t1, t2, t2, t3, t3, t4, t4]\n    sin_pos = torch.stack([sin, sin], dim=-1).view(S, D)\n    cos_pos = torch.stack([cos, cos], dim=-1).view(S, D)\n\n    # 2nd term: [-x2, x1, -x4, x3, -x6, x5, -x8, x7]\n    x_half = torch.stack([-x[..., 1::2], x[..., 0::2]], dim=-1).reshape_as(x)\n\n    x_rot = x * cos_pos + x_half * sin_pos\n    return x_rot\n\n\nx_rotated = apply_rope34(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\nThe above code is similar to the RoFormer implementation in the transformers library.\n\n\nSimplified 2D Implementation (based on equation 13)\n\n\n\nimage.png\n\n\nIf we assume that the embedding \\(x\\) is already multiplied with the \\(Q\\) and \\(K\\) matrices, then we can simplify the above equation as follows:\n\\[\\begin{align}\n    f(x_m, m) = \\begin{pmatrix}\n    \\cos m\\theta & \\sin m\\theta \\\\\n    \\sin m\\theta & \\cos m\\theta\n    \\end{pmatrix} \\begin{pmatrix}\n    x_1 \\\\ x_2\n    \\end{pmatrix}\n    &= \\begin{pmatrix}\n    x_1 * \\cos m\\theta - x_2 * \\sin m\\theta \\\\\n    x_1 * \\sin m\\theta - x_2 * \\cos m\\theta\n    \\end{pmatrix}\n\\end{align}\\]\nHere‚Äôs what the code implementation looks like:\n\ndef apply_rope13(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # [batch, seq_len, dim//2]\n    sin, cos = theta.sin(), theta.cos()\n\n    # even and odd terms\n    x1, x2 = x[..., 0::2], x[..., 1::2]\n\n    # [cos_nŒ∏, -sin_nŒ∏] [x1]\n    # [sin_nŒ∏,  cos_nŒ∏] [x2]\n    # =&gt; [x1 * cos_nŒ∏ - x2 * sin_nŒ∏, x1 * sin_nŒ∏ + x2 * cos_nŒ∏]\n    x_rot = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1).reshape_as(\n        x\n    )\n    return x_rot\n\n\nx_rotated = apply_rope13(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\n\n# Both implementation should produce same result\nassert torch.allclose(apply_rope34(x), apply_rope13(x)), \"Not Equal\"\n\nIn this approach, we don‚Äôt have to duplicate theta. Ideally, this should save some memory. The official RoFormer PyTorch repo inspired the above implementation.\n\n\nLlama3.1 Implementation\nMost RoPE implementations I found online makes use one of the two implementations above. But RoPE implementation in Meta Llama3.1 implements RoPE directly using equation 12 with complex number mathematics. While It might sound complex, but the code is pretty concise.\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    ndim = x.ndim\n    assert 0 &lt;= 1 &lt; ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\n\ndef apply_rope_llama31(x):\n    _, S, D = x.shape\n\n    # [mŒ∏1, mŒ∏2, mŒ∏3, mŒ∏4, . . . mŒ∏(d//2)] | Shape: [seq_len, dim//2]\n    theta = get_theta(S, D)\n\n    # constructs a complex tensor from polar coordinates\n    freq_cis = torch.polar(torch.ones_like(theta), theta)\n\n    # turn `x` into complex number\n    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n    freq_cis = reshape_for_broadcast(freq_cis, x_)\n\n    # Real[X * e^(imŒ∏)]\n    x_rotated = torch.view_as_real(x_ * freq_cis).flatten(-2)\n    return x_rotated\n\n\nx_rotated = apply_rope_llama31(x)\nprint(f\"{x.shape=}\")\nprint(f\"{x_rotated.shape=}\")  # same shape as `x`\n\n\n# should be equal to other two implementations\nassert torch.allclose(apply_rope_llama31(x), apply_rope13(x)), \"Not Equal\"\nassert torch.allclose(apply_rope_llama31(x), apply_rope34(x)), \"Not Equal\"\n\nThe complete code can be found here: Llama3.1 RoPE implementation. Also, Umar Jamil has this amazing YouTube video where he codes Llama2 from scratch, implementing RoPE step-by-step.\nThe last objective was to check out the HF implementation and call it a day. But I didn‚Äôt know what I was stepping into.\n\n\nHuggingFace implementation\nI decided to look at the RoPE implementation in the Llama model. Their implementation was very similar to our equation 34 implementation, where we duplicated theta, calculated cos, sin, and x_half, and finally combined them to derive x_rotated.\nBut there was one subtle difference in how they calculated x_half. Here is the code snippet:\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\nInstead of taking even and odd indexes, they simply take the first and second halves. My initial thought was, ‚ÄúThis seems wrong,‚Äù but if that were the case, others would have noticed it. Maybe I‚Äôm missing something. So, I extracted the relevant parts and compared them with my implementation. The results didn‚Äôt match.\nMy next hunch was that they might have used some combination of reshape, transpose, and broadcast to do it and still get the same results. But why? My guess was performance!\nI started hunting for these magic transformations in the codebase. After wasting a couple of hours looking through the code, copy-pasting, writing code, and seeking help from LLMs (conversation link: ChatGPT & Claude), I became sure that the HF implementation was wrong (even though it‚Äôs very unlikely).\nThe next step was to create an issue, but being a good open-source contributor and to avoid duplication, I decided to first search for it. I realized that many people before me had the same realization. Here is one such issue with an in-depth discussion and numerous resources. TL;DR: comment.\nBasically, when adding any model to the Transformers library, one has to include a convert_xxxx_weights_to_hf.py script. This script converts the original weight and layer names to fit the HF convention. The common belief is that weights should not be changed. But the convert script for Llama has a premute function that rearranges the weights:\n# permute for sliced rotary\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\nThis re-arrangement is what allows them to use x[..., :D/2] and x[..., D/2:] in the RoPE operation, rather than splitting (x) by even and odd indices. Therefore, my guess was right‚Äîthey do have the magic transformation that I was looking for, but it‚Äôs not in the training/inference code, it‚Äôs in the convert script. This can be a source of confusion, as it‚Äôs not very well documented and is located in a completely different part of the codebase.\nThis discovery was a fantastic outcome that truly validated my understanding of RoPE. It was a subtle, latent finding that confirmed my thought process and added a layer of depth to my comprehension. The whole journey was a thrilling adventure filled with learning, and I thoroughly enjoyed the process. It‚Äôs a perfect example of how diving deep into a problem not only strengthens your expertise but also makes the journey of discovery incredibly fun."
  },
  {
    "objectID": "nbs/Rotary_Position_Embedding.html#building-intuition-about-rope",
    "href": "nbs/Rotary_Position_Embedding.html#building-intuition-about-rope",
    "title": "Understanding LLMs",
    "section": "Building intuition about RoPE",
    "text": "Building intuition about RoPE\nNote: This section was added later, on April 13th, 2025.\nHaving a good intuition about RoPE is really important for understanding various RoPE-based LLM context window extension methods. After spending few weeks going through different papers, implementing them, and pulling my hairs out for days to make of it all, I found that visualizing the theta matrix was one of the most effective ways to understand them. Visualization turned out to be a game changer in this quest.\nLet‚Äôs say your embedding vector \\((X)\\), for a token, is of shape \\([1, 16]\\). RoPE introduces a rotation vector \\((\\theta)\\) which is half the size of \\(X\\) i.e., \\([1, 8]\\). The values in \\(X\\) are rotated in pairs of two ‚Äì so \\(x_1\\) and \\(x_2\\) will be rotated by angle \\(\\theta_1\\). Similarly, other pairs will be rotated by their corresponding angles. I have color-coded the elements of \\(X\\) and \\(\\theta\\) to make it easy to visualize.\n\n\n\nimage.png\n\n\nSo, the first and obvious question is: Where do we get \\(\\theta\\)? Simple, we calculate it using the formula: \\[\\theta_d = b^{-2d/|D|}\\] Here, \\(b\\) is called base (generally set to 10,000), and \\(|D|\\) is dimensionality of \\(X\\) (which is 16 in our example).\nBelow is the pytorch code to calculate the \\(\\theta\\) vector:\n\nemb_dim = 16\nbase = 10_000\n\ntheta = 1.0 / (base ** (torch.arange(0, emb_dim, 2) / emb_dim))\ntheta\n\nLets plot our theta values\n\nplt.plot(theta)\nplt.title(\"Theta Vector\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Theta value\")\nplt.show()\n\nHere‚Äôs how to interpret the plot: - Y-axis shows the actual value of \\(Œ∏\\) (theta) for each index. - From left to right, the \\(Œ∏\\) value decay exponentially ‚Äì lower dimensions are assigned higher frequencies (large \\(Œ∏\\) values), while higher dimension receive lower frequencies. - The rotation angle becomes nearly zero for higher dimensions. In the plot above, indices 5, 6, and 7 are all approximately zero.\nBeing able to simply calculate these values is what makes RoPE flexible for extending context window. Since these values are not learnt, we can change them even after training allowing us to easily extend the context window of pre-trained LLMs that use RoPE.\n\nProcessing Sequence\nMoving on, when processing a sequence, we have multiple tokens. To encode positional information, we multiply the rotation vector \\(Œ∏\\) by the position of each token in the sequence.\nAs shown in the image below, the same rotation vector \\((\\theta)\\) is scaled (i.e.¬†multiplied) by the position of the token in the sequence. This results in a 2D matrix, where each value represents the angle by which a corresponding pair of embedding values will be rotated. Notice, each row of this matrix (i.e., each token position) contains distinct angles, enabling the model to differentiate ‚Äì and thus learn ‚Äì based on token position.\n\n\n\nimage.png\n\n\nHere is the pytorch code for generating the matrix\n\nmax_seq_len = 8\n\npos_ids = torch.arange(0, max_seq_len)\nthetas = torch.outer(pos_ids, theta)\nthetas.shape\n\nWe have a theta vector that is 8D, and we can have at most 8 tokens in the sequence. Plotting thetas matrix:\n\nfor i, t in enumerate(thetas):\n    plt.plot(t, label=f\"Position {i}\")\nplt.title(\"Theta Vector\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Theta value\")\nplt.legend()\nplt.show()\n\nAs the token position in the sequence increases, the angle of rotation also increases linearly.\nHowever, theta values for the higher dimensions (indices 5, 6, and 7) are almost indistinguishable across different token positions. This effect is much clearer when visualized using in a heatmap.\n\nplt.imshow(thetas, cmap=\"Blues\")\nplt.colorbar()\nplt.title(\"Theta Matrix\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Token Position\")\nplt.show()\n\nWow, most values in the matrix are close to zero!\nWhat you‚Äôre looking at is a 2D matrix where:\n\nGoing left to right (i.e., as the index of the theta vector increases), the values decrease.\nGoing top to bottom (i.e., as the token position increases), the values increase.\n\nThis pattern is quite important to understand. So, here‚Äôs a quick question to test your grasp of it:\nWhere is the smallest value and where is the largest value in the matrix?\nNow, scroll back up, and take a closer look at get_theta function defined above. At this point, it should be a piece of cake to both understand and even write this function from scratch!"
  },
  {
    "objectID": "nbs/Rotary_Position_Embedding.html#further-work",
    "href": "nbs/Rotary_Position_Embedding.html#further-work",
    "title": "Understanding LLMs",
    "section": "Further Work",
    "text": "Further Work\nNow that we understand the mechanics of RoPE embeddings, I plan to dive deeper into various methods of extending the context window in LLMs that are built on top of RoPE. Specifically, I want to better understand the how theta values are manipulated to extend context window. The goal is to develop a more comprehensive understanding of RoPE by exploring it from multiple perspectives. I believe this will provide valuable insights and make it easier to further deepen my knowledge in this area."
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "LLM From Scratch: Part 2\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nAnkur Singh\n\n3 min\n\n\n\n\n\n\nLLM From Scratch: Part 1\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nAug 6, 2025\n\n\nAnkur Singh\n\n2 min\n\n\n\n\n\n\nRotary Position Embedding (RoPE)\n\n\n\n\n\n\nLLM\n\nRoPE\n\n\n\n\n\n\n\n\n\nJul 25, 2025\n\n\nAnkur Singh\n\n12 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "nbs/LLM_from_Scratch_1.html",
    "href": "nbs/LLM_from_Scratch_1.html",
    "title": "Model Architecture",
    "section": "",
    "text": "To create a minimal GPT-style model, you need these core components:\n\nModel Architecture: Defines how tokens are processed and contextual relationships are modeled.\nInference (Next Token Generation): Uses the trained model to generate the next token from input tokens.\nTraining Data: A tokenized text dataset for training the model.\nTraining Loop: Iteratively updates model parameters to minimize prediction error.\n\nEach component will be implemented simply for clarity. Later notebooks will introduce improvements and optimizations.\n\n!pip install -Uq torch\n!pip install -Uq datasets tiktoken\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Misc\nimport math\nimport tiktoken\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom dataclasses import dataclass\nfrom prettytable import PrettyTable\n\n\nModel Architecture\nWe will start by first defining the model architecture and try to generate some text to make sure everything is working as expected\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.context = context\n        self.mha = nn.MultiheadAttention(emb_dim, heads, batch_first=True)\n        self.proj = nn.Linear(emb_dim, emb_dim)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n        seq_len = min(seq_len, self.context)\n        attn_mask = self.mask[:seq_len, :seq_len]\n        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n        return self.proj(attn_out)\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, 4 * emb_dim), nn.GELU(), nn.Linear(4 * emb_dim, emb_dim)\n        )\n        self.sa_norm = nn.LayerNorm(emb_dim)\n        self.mlp_norm = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.sa_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(config.context, config.emb_dim)\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.LayerNorm(config.emb_dim)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        pos = torch.arange(seq_len, device=x.device)\n        x = self.tok_embedding(x) + self.pos_embedding(pos)\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\n@dataclass\nclass ModelConfig:\n    # GPT2 architecture\n    vocab: int = math.ceil(50_257 / 64) * 64  # nearest multiple of 64\n    emb_dim: int = 768\n    heads: int = 12\n    layers: int = 12\n    context: int = 1024\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\n# Utility Function: Number of Trainable Parameters\ndef count_parameters(model, verbose=False):\n    if verbose:\n        table = PrettyTable([\"Module\", \"Parameters\"])\n        total = 0\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                count = param.numel()\n                table.add_row([name, count])\n                total += count\n        print(table)\n    else:\n        total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Params: {total / 1e6:.2f} M\")\n\n\ncount_parameters(model)\n\nBased on my calculations, this looks good.\n\nNote: This is not exactly save as GPT2 (124M). That is because of no weight-tying and other small difference. Read this to learn more about weight tying.\n\n\n\nInference (Next Token Generation)\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        next_idx = torch.argmax(logits, dim=-1, keepdim=True)\n        prefix += tokenizer.decode([next_idx.cpu()])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\nprefix = \"Once upon a time\"\nprint(generate(model, tokenizer, prefix))\n\nThe generated text is gibberish because the model is not trained yet.\n\nNote: You will get the same output each time you run the cell, since there is no randomness in the sampling process. The model is initialized with random weights. To get different outputs, you must reinitialize the model.\n\n\n\nData\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ndataset\n\n\nval_ds = \"\\n\\n\".join(dataset[\"test\"][\"text\"])\ntrain_ds = \"\\n\\n\".join(dataset[\"train\"][\"text\"])\n\nval_tokens = tokenizer.encode(val_ds)\ntrain_tokens = tokenizer.encode(train_ds)\nlen(val_tokens), len(train_tokens)\n\n\nclass WikiTextDataset(Dataset):\n    def __init__(self, tokens, max_len):\n        self.tokens = tokens\n        self.max_len = max_len\n\n    def __getitem__(self, idx):\n        idx = idx * self.max_len\n        x = self.tokens[idx : idx + self.max_len]\n        y = self.tokens[idx + 1 : idx + 1 + self.max_len]\n        if len(x) &lt; self.max_len:\n            x = x + [tokenizer.eot_token] * (self.max_len - len(x))\n        if len(y) &lt; self.max_len:\n            y = y + [tokenizer.eot_token] * (self.max_len - len(y))\n        return (torch.tensor(x), torch.tensor(y))\n\n    def __len__(self):\n        return math.ceil(len(self.tokens) / self.max_len)\n\n\nval_ds = WikiTextDataset(val_tokens, ModelConfig.context)\ntrain_ds = WikiTextDataset(train_tokens, ModelConfig.context)\nlen(val_ds), len(train_ds)\n\n\nbatch_size = 6\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n\n\nnext(iter(val_dl))\n\n\n\nTraining Loop\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nmodel.to(device)\nevaluate(model, val_dl)\n\nThis looks correct. Initially, the probability will be evenly distributed, i.e., each token will roughly have the same probability. As a result, we can calculate the expected value of the loss: -ln(1/50304) ‚âà 10.826.\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n\nlog_freq = 40\nepochs = 2\nlosses = []\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n\nInference\nLets load the saved model and try to generate some sample text . . .\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\nThe generated text is not very good. Let‚Äôs add some randomness.\nInstead of always picking the highest probability, we can sample the next token index from the probability distribution. This involves two steps: 1. Convert logits to probabilities. 2. Sample the next token index from this distribution.\nNote: Sampling adds randomness, so you will see different outputs each time you run the cell.\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits, dim=-1)  # &lt;-- update\n        next_idx = torch.multinomial(probs, num_samples=1)  # &lt;-- update\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\ntemperature is a useful parameter that controls how sharp or flat the softmax output is.\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits / temp, dim=-1)  # &lt;-- update: scale using temp\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\nHere‚Äôs why temperature affects creativity:\n\nimport matplotlib.pyplot as plt\n\nlogits = torch.randn(4, 32)\nplt.plot(torch.softmax(logits[0], dim=-1), label=\"No Temperature\")\nplt.plot(torch.softmax(logits[0] / 0.5, dim=-1), label=\"0.5 Temperature\")\nplt.plot(torch.softmax(logits[0] / 4, dim=-1), label=\"2 Temperature\")\nplt.legend()\n\nAs shown: - If temperature is low (&lt; 1), softmax is sharp and only a few tokens have high probability. - If temperature is high (&gt; 1), softmax is flatter and more tokens have similar probabilities.\nAnother improvement is to sample only from the top-k probabilities.\n\ndef topk(logits, k=5):\n    topk_vals, topk_idxs = torch.topk(logits, k)\n    probs = torch.zeros_like(logits)\n    probs[:, topk_idxs] = torch.softmax(topk_vals, dim=-1)\n    return probs\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = topk(logits / temp)  # &lt;-- update: only `topk` probabilities\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\nLet‚Äôs try these improvements and see how the generated text changes.\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Understanding LLMs",
    "section": "",
    "text": "A hands-on collection of Jupyter notebooks exploring key concepts, architectures, and optimizations in Large Language Models (LLMs).\n\nNotebooks\n\nLLM from Scratch - Part 1 \n\nBuild a basic LLM using PyTorch.\nCovers data preprocessing, model architecture, training loop, and inference.\n\nLLM from Scratch - Part 2 \n\nEnhances the previous notebook with modern techniques:\n\nRMSNorm\nGated (SwiGLU) FFN\nRotary Positional Embeddings (RoPE)\n\nIncludes gradient accumulation and mixed precision training.\n\nRotary Positional Embeddings (RoPE) \n\nDeep dive into RoPE in PyTorch.\nCompares different RoPE implementations."
  },
  {
    "objectID": "notebooks/LLM_from_Scratch_1.html",
    "href": "notebooks/LLM_from_Scratch_1.html",
    "title": "LLM From Scratch: Part 1",
    "section": "",
    "text": "To create a minimal GPT-style model, you need these core components:\n\nModel Architecture: Defines how tokens are processed and contextual relationships are modeled.\nInference (Next Token Generation): Uses the trained model to generate the next token from input tokens.\nTraining Data: A tokenized text dataset for training the model.\nTraining Loop: Iteratively updates model parameters to minimize prediction error.\n\nEach component will be implemented simply for clarity. Later notebooks will introduce improvements and optimizations.\n\n!pip install -Uq torch\n!pip install -Uq datasets tiktoken\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Misc\nimport math\nimport tiktoken\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom dataclasses import dataclass\nfrom prettytable import PrettyTable\n\n\nModel Architecture\nWe will start by first defining the model architecture and try to generate some text to make sure everything is working as expected\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n        self.context = context\n        self.mha = nn.MultiheadAttention(emb_dim, heads, batch_first=True)\n        self.proj = nn.Linear(emb_dim, emb_dim)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n        )\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n        seq_len = min(seq_len, self.context)\n        attn_mask = self.mask[:seq_len, :seq_len]\n        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n        return self.proj(attn_out)\n\n\nclass Block(nn.Module):\n    def __init__(self, emb_dim, heads, context):\n        super().__init__()\n        self.mha = MultiheadAttention(emb_dim, heads, context)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, 4 * emb_dim), nn.GELU(), nn.Linear(4 * emb_dim, emb_dim)\n        )\n        self.sa_norm = nn.LayerNorm(emb_dim)\n        self.mlp_norm = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        x = x + self.mha(self.sa_norm(x))\n        x = x + self.mlp(self.mlp_norm(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(config.context, config.emb_dim)\n        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n        self.decoder = nn.Sequential(\n            *[\n                Block(config.emb_dim, config.heads, config.context)\n                for _ in range(config.layers)\n            ]\n        )\n        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n        self.norm = nn.LayerNorm(config.emb_dim)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n        pos = torch.arange(seq_len, device=x.device)\n        x = self.tok_embedding(x) + self.pos_embedding(pos)\n        x = self.decoder(x)\n        return self.output(self.norm(x))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n\n@dataclass\nclass ModelConfig:\n    # GPT2 architecture\n    vocab: int = math.ceil(50_257 / 64) * 64  # nearest multiple of 64\n    emb_dim: int = 768\n    heads: int = 12\n    layers: int = 12\n    context: int = 1024\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = GPT(ModelConfig)\nmodel = model.to(device)\n\n\n# Utility Function: Number of Trainable Parameters\ndef count_parameters(model, verbose=False):\n    if verbose:\n        table = PrettyTable([\"Module\", \"Parameters\"])\n        total = 0\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                count = param.numel()\n                table.add_row([name, count])\n                total += count\n        print(table)\n    else:\n        total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total Trainable Params: {total / 1e6:.2f} M\")\n\n\ncount_parameters(model)\n\nBased on my calculations, this looks good.\n\nNote: This is not exactly save as GPT2 (124M). That is because of no weight-tying and other small difference. Read this to learn more about weight tying.\n\n\n\nInference (Next Token Generation)\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        next_idx = torch.argmax(logits, dim=-1, keepdim=True)\n        prefix += tokenizer.decode([next_idx.cpu()])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\nprefix = \"Once upon a time\"\nprint(generate(model, tokenizer, prefix))\n\nThe generated text is gibberish because the model is not trained yet.\n\nNote: You will get the same output each time you run the cell, since there is no randomness in the sampling process. The model is initialized with random weights. To get different outputs, you must reinitialize the model.\n\n\n\nData\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ndataset\n\n\nval_ds = \"\\n\\n\".join(dataset[\"test\"][\"text\"])\ntrain_ds = \"\\n\\n\".join(dataset[\"train\"][\"text\"])\n\nval_tokens = tokenizer.encode(val_ds)\ntrain_tokens = tokenizer.encode(train_ds)\nlen(val_tokens), len(train_tokens)\n\n\nclass WikiTextDataset(Dataset):\n    def __init__(self, tokens, max_len):\n        self.tokens = tokens\n        self.max_len = max_len\n\n    def __getitem__(self, idx):\n        idx = idx * self.max_len\n        x = self.tokens[idx : idx + self.max_len]\n        y = self.tokens[idx + 1 : idx + 1 + self.max_len]\n        if len(x) &lt; self.max_len:\n            x = x + [tokenizer.eot_token] * (self.max_len - len(x))\n        if len(y) &lt; self.max_len:\n            y = y + [tokenizer.eot_token] * (self.max_len - len(y))\n        return (torch.tensor(x), torch.tensor(y))\n\n    def __len__(self):\n        return math.ceil(len(self.tokens) / self.max_len)\n\n\nval_ds = WikiTextDataset(val_tokens, ModelConfig.context)\ntrain_ds = WikiTextDataset(train_tokens, ModelConfig.context)\nlen(val_ds), len(train_ds)\n\n\nbatch_size = 6\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n\n\nnext(iter(val_dl))\n\n\n\nTraining Loop\n\n@torch.no_grad()\ndef evaluate(model, dl):\n    model.eval()\n    loss = 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n    model.train()\n    return loss / len(dl)\n\n\nmodel.to(device)\nevaluate(model, val_dl)\n\nThis looks correct. Initially, the probability will be evenly distributed, i.e., each token will roughly have the same probability. As a result, we can calculate the expected value of the loss: -ln(1/50304) ‚âà 10.826.\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n\nlog_freq = 40\nepochs = 2\nlosses = []\n\nfor epoch in range(epochs):\n    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n        if i % log_freq == 0:\n            val_loss = evaluate(model, val_dl)\n            losses.append(val_loss)\n            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n            torch.save(model.state_dict(), \"model.pth\")\n            print(\"=\" * 20)\n            print(generate(model, tokenizer, prefix))\n\n        model.train()\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n\nInference\nLets load the saved model and try to generate some sample text . . .\n\nstate_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\nThe generated text is not very good. Let‚Äôs add some randomness.\nInstead of always picking the highest probability, we can sample the next token index from the probability distribution. This involves two steps: 1. Convert logits to probabilities. 2. Sample the next token index from this distribution.\nNote: Sampling adds randomness, so you will see different outputs each time you run the cell.\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits, dim=-1)  # &lt;-- update\n        next_idx = torch.multinomial(probs, num_samples=1)  # &lt;-- update\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))\n\ntemperature is a useful parameter that controls how sharp or flat the softmax output is.\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = torch.softmax(logits / temp, dim=-1)  # &lt;-- update: scale using temp\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\nHere‚Äôs why temperature affects creativity:\n\nimport matplotlib.pyplot as plt\n\nlogits = torch.randn(4, 32)\nplt.plot(torch.softmax(logits[0], dim=-1), label=\"No Temperature\")\nplt.plot(torch.softmax(logits[0] / 0.5, dim=-1), label=\"0.5 Temperature\")\nplt.plot(torch.softmax(logits[0] / 4, dim=-1), label=\"2 Temperature\")\nplt.legend()\n\nAs shown: - If temperature is low (&lt; 1), softmax is sharp and only a few tokens have high probability. - If temperature is high (&gt; 1), softmax is flatter and more tokens have similar probabilities.\nAnother improvement is to sample only from the top-k probabilities.\n\ndef topk(logits, k=5):\n    topk_vals, topk_idxs = torch.topk(logits, k)\n    probs = torch.zeros_like(logits)\n    probs[:, topk_idxs] = torch.softmax(topk_vals, dim=-1)\n    return probs\n\n\n@torch.no_grad()\ndef generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n    model.eval()\n    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        logits = model(token_ids)\n        logits = logits[:, -1, :]\n        probs = topk(logits / temp)  # &lt;-- update: only `topk` probabilities\n        next_idx = torch.multinomial(probs, num_samples=1)\n        prefix += tokenizer.decode([next_idx])\n        token_ids = torch.cat((token_ids, next_idx), dim=1)\n    return prefix\n\nLet‚Äôs try these improvements and see how the generated text changes.\n\nprint(generate(model, tokenizer, \"Once upon a time\"))\n\n\nprint(generate(model, tokenizer, \"Internet is an\"))\n\n\nprint(generate(model, tokenizer, \"AI will\"))\n\n\nprint(generate(model, tokenizer, \"The meaning of life is\"))"
  }
]