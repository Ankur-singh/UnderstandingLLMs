{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/nbs/LLL_from_Scratch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a minimal GPT-style model, you need these core components:\n",
    "\n",
    "* **Model Architecture**: Defines how tokens are processed and contextual relationships are modeled.\n",
    "* **Inference (Next Token Generation)**: Uses the trained model to generate the next token from input tokens.\n",
    "* **Training Data**: A tokenized text dataset for training the model.\n",
    "* **Training Loop**: Iteratively updates model parameters to minimize prediction error.\n",
    "\n",
    "Each component will be implemented simply for clarity. Later notebooks will introduce improvements and optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq torch\n",
    "!pip install -Uq datasets tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Misc\n",
    "import math\n",
    "import tiktoken\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by first defining the model architecture and try to generate some text to make sure everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, context):\n",
    "        super().__init__()\n",
    "        assert emb_dim % heads == 0, \"`emb_dim` should be a multiple of `heads`\"\n",
    "        self.context = context\n",
    "        self.mha = nn.MultiheadAttention(emb_dim, heads, batch_first=True)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context, context), diagonal=1).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        seq_len = min(seq_len, self.context)\n",
    "        attn_mask = self.mask[:seq_len, :seq_len]\n",
    "        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        return self.proj(attn_out)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, context):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(emb_dim, heads, context)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim), nn.GELU(), nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.sa_norm = nn.LayerNorm(emb_dim)\n",
    "        self.mlp_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.sa_norm(x))\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config.context, config.emb_dim)\n",
    "        self.tok_embedding = nn.Embedding(config.vocab, config.emb_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[\n",
    "                Block(config.emb_dim, config.heads, config.context)\n",
    "                for _ in range(config.layers)\n",
    "            ]\n",
    "        )\n",
    "        self.output = nn.Linear(config.emb_dim, config.vocab, bias=False)\n",
    "        self.norm = nn.LayerNorm(config.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len = x.shape\n",
    "        pos = torch.arange(seq_len, device=x.device)\n",
    "        x = self.tok_embedding(x) + self.pos_embedding(pos)\n",
    "        x = self.decoder(x)\n",
    "        return self.output(self.norm(x))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # GPT2 architecture\n",
    "    vocab: int = math.ceil(50_257 / 64) * 64  # nearest multiple of 64\n",
    "    emb_dim: int = 768\n",
    "    heads: int = 12\n",
    "    layers: int = 12\n",
    "    context: int = 1024\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT(ModelConfig)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function: Number of Trainable Parameters\n",
    "def count_parameters(model, verbose=False):\n",
    "    if verbose:\n",
    "        table = PrettyTable([\"Module\", \"Parameters\"])\n",
    "        total = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                count = param.numel()\n",
    "                table.add_row([name, count])\n",
    "                total += count\n",
    "        print(table)\n",
    "    else:\n",
    "        total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Trainable Params: {total / 1e6:.2f} M\")\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my calculations, this looks good.\n",
    "\n",
    "> **Note:** This is not exactly save as GPT2 (124M). That is because of **no weight-tying** and other small difference. Read [this](https://www.perplexity.ai/search/weight-tying-gpt2-bgeVq5MzTbC2v8d5NEFx_g) to learn more about weight tying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference (Next Token Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prefix, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        logits = logits[:, -1, :]\n",
    "        next_idx = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        prefix += tokenizer.decode([next_idx.cpu()])\n",
    "        token_ids = torch.cat((token_ids, next_idx), dim=1)\n",
    "    return prefix\n",
    "\n",
    "\n",
    "prefix = \"Once upon a time\"\n",
    "print(generate(model, tokenizer, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text is gibberish because the model is not trained yet.\n",
    "\n",
    "> **Note**: You will get the same output each time you run the cell, since there is no randomness in the sampling process. The model is initialized with random weights. To get different outputs, you must reinitialize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = \"\\n\\n\".join(dataset[\"test\"][\"text\"])\n",
    "train_ds = \"\\n\\n\".join(dataset[\"train\"][\"text\"])\n",
    "\n",
    "val_tokens = tokenizer.encode(val_ds)\n",
    "train_tokens = tokenizer.encode(train_ds)\n",
    "len(val_tokens), len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, tokens, max_len):\n",
    "        self.tokens = tokens\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx * self.max_len\n",
    "        x = self.tokens[idx : idx + self.max_len]\n",
    "        y = self.tokens[idx + 1 : idx + 1 + self.max_len]\n",
    "        if len(x) < self.max_len:\n",
    "            x = x + [tokenizer.eot_token] * (self.max_len - len(x))\n",
    "        if len(y) < self.max_len:\n",
    "            y = y + [tokenizer.eot_token] * (self.max_len - len(y))\n",
    "        return (torch.tensor(x), torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.tokens) / self.max_len)\n",
    "\n",
    "\n",
    "val_ds = WikiTextDataset(val_tokens, ModelConfig.context)\n",
    "train_ds = WikiTextDataset(train_tokens, ModelConfig.context)\n",
    "len(val_ds), len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss += F.cross_entropy(logits.flatten(0, 1), y.flatten()).cpu().item()\n",
    "    model.train()\n",
    "    return loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct. Initially, the probability will be evenly distributed, i.e., each token will roughly have the same probability. As a result, we can calculate the expected value of the loss: `-ln(1/50304) ≈ 10.826`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_freq = 40\n",
    "epochs = 2\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(pbar := tqdm(train_dl, desc=\"Training\")):\n",
    "        if i % log_freq == 0:\n",
    "            val_loss = evaluate(model, val_dl)\n",
    "            losses.append(val_loss)\n",
    "            pbar.set_postfix_str(f\"[Epoch {epoch}] Val Loss: {val_loss:.3f}\")\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "            print(\"=\" * 20)\n",
    "            print(generate(model, tokenizer, prefix))\n",
    "\n",
    "        model.train()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the saved model and try to generate some sample text . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Once upon a time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Internet is an\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"AI will\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"The meaning of life is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text is not very good. Let's add some randomness.\n",
    "\n",
    "Instead of always picking the highest probability, we can sample the next token index from the probability distribution. This involves two steps:\n",
    "1. Convert `logits` to probabilities.\n",
    "2. Sample the next token index from this distribution.\n",
    "\n",
    "**Note**: Sampling adds randomness, so you will see different outputs each time you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prefix, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)  # <-- update\n",
    "        next_idx = torch.multinomial(probs, num_samples=1)  # <-- update\n",
    "        prefix += tokenizer.decode([next_idx])\n",
    "        token_ids = torch.cat((token_ids, next_idx), dim=1)\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Once upon a time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Internet is an\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"AI will\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"The meaning of life is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`temperature` is a useful parameter that controls how sharp or flat the softmax output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n",
    "    model.eval()\n",
    "    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits / temp, dim=-1)  # <-- update: scale using temp\n",
    "        next_idx = torch.multinomial(probs, num_samples=1)\n",
    "        prefix += tokenizer.decode([next_idx])\n",
    "        token_ids = torch.cat((token_ids, next_idx), dim=1)\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's why `temperature` affects creativity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logits = torch.randn(4, 32)\n",
    "plt.plot(torch.softmax(logits[0], dim=-1), label=\"No Temperature\")\n",
    "plt.plot(torch.softmax(logits[0] / 0.5, dim=-1), label=\"0.5 Temperature\")\n",
    "plt.plot(torch.softmax(logits[0] / 4, dim=-1), label=\"2 Temperature\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown:\n",
    "- If `temperature` is low (< 1), softmax is sharp and only a few tokens have high probability.\n",
    "- If `temperature` is high (> 1), softmax is flatter and more tokens have similar probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another improvement is to sample only from the top-k probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(logits, k=5):\n",
    "    topk_vals, topk_idxs = torch.topk(logits, k)\n",
    "    probs = torch.zeros_like(logits)\n",
    "    probs[:, topk_idxs] = torch.softmax(topk_vals, dim=-1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prefix, max_new_tokens=10, temp=1.0):\n",
    "    model.eval()\n",
    "    token_ids = torch.tensor(tokenizer.encode(prefix), device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = topk(logits / temp)  # <-- update: only `topk` probabilities\n",
    "        next_idx = torch.multinomial(probs, num_samples=1)\n",
    "        prefix += tokenizer.decode([next_idx])\n",
    "        token_ids = torch.cat((token_ids, next_idx), dim=1)\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these improvements and see how the generated text changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Once upon a time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"Internet is an\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"AI will\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, \"The meaning of life is\"))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
