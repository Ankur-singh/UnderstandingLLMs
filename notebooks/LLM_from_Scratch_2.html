<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ankur Singh">
<meta name="dcterms.date" content="2025-08-15">

<title>LLM From Scratch: Part 2 – Understanding LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Understanding LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../notebooks.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.github.com/Ankur-singh"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.twitter.com/I_ankursingh"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">LLM From Scratch: Part 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Ankur Singh </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><a href="https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/nbs/LLM_from_Scratch_2.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p>In this notebook, we incorporate architectural improvements to the LLM from the previous part. While code changes are minor, understanding the rationale is crucial. We’ll discuss these changes and provide helpful resources:</p>
<ul>
<li>RMSNorm (instead of LayerNorm)</li>
<li>Pre-LN and Post-LN</li>
<li>Gated FFN - SwiGLU</li>
<li>RoPE</li>
<li>Minor Attention block changes will be discussed in a separate notebook for comparison (Sliding Window, Multimodal, MoE, etc.).</li>
</ul>
<section id="setup" class="level1">
<h1>Setup</h1>
<p>The model architecture is copied directly from Part 1.</p>
<div id="cell-5" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>Uq torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>Uq datasets tiktoken</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Misc</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> prettytable <span class="im">import</span> PrettyTable</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiheadAttention(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, heads, context):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> emb_dim <span class="op">%</span> heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"`emb_dim` should be a multiple of `heads`"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context <span class="op">=</span> context</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> nn.MultiheadAttention(emb_dim, heads, batch_first<span class="op">=</span><span class="va">True</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mask"</span>, torch.triu(torch.ones(context, context), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        batch, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> <span class="bu">min</span>(seq_len, <span class="va">self</span>.context)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        attn_mask <span class="op">=</span> <span class="va">self</span>.mask[:seq_len, :seq_len]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.mha(x, x, x, attn_mask<span class="op">=</span>attn_mask, need_weights<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_out</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, heads, context):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultiheadAttention(emb_dim, heads, context)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(emb_dim, <span class="dv">4</span> <span class="op">*</span> emb_dim), nn.GELU(), nn.Linear(<span class="dv">4</span> <span class="op">*</span> emb_dim, emb_dim)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha_norm <span class="op">=</span> nn.LayerNorm(emb_dim)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp_norm <span class="op">=</span> nn.LayerNorm(emb_dim)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha(<span class="va">self</span>.mha_norm(x))</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.mlp_norm(x))</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(config.context, config.emb_dim)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embedding <span class="op">=</span> nn.Embedding(config.vocab, config.emb_dim)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>[</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>                Block(config.emb_dim, config.heads, config.context)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.layers)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(config.emb_dim, config.vocab, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(config.emb_dim)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        batch, seq_len <span class="op">=</span> x.shape</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        pos <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>x.device)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tok_embedding(x) <span class="op">+</span> <span class="va">self</span>.pos_embedding(pos)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.decoder(x)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output(<span class="va">self</span>.norm(x))</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> device(<span class="va">self</span>):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">next</span>(<span class="va">self</span>.parameters()).device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Utility Function: Number of Trainable Parameters</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        table <span class="op">=</span> PrettyTable([<span class="st">"Module"</span>, <span class="st">"Parameters"</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param.requires_grad:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                count <span class="op">=</span> param.numel()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                table.add_row([name, count])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                total <span class="op">+=</span> count</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(table)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total Trainable Params: </span><span class="sc">{</span>total <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss"> M"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelConfig:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GPT2 architecture</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    vocab: <span class="bu">int</span> <span class="op">=</span> math.ceil(<span class="dv">50_257</span> <span class="op">/</span> <span class="dv">64</span>) <span class="op">*</span> <span class="dv">64</span>  <span class="co"># nearest multiple of 64</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    emb_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    context: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-10" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT(ModelConfig)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>count_parameters(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="rmsnorm" class="level1">
<h1>1. RMSNorm</h1>
<p>This is a simple change: replace <code>nn.LayerNorm</code> with <code>nn.RMSNorm</code>, which applies Root Mean Square Layer Normalization over a mini-batch of inputs.</p>
<ul>
<li>Learn more from the <a href="https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html">PyTorch RMSNorm docs</a>.</li>
<li>For deeper insights and empirical results, see the <a href="https://arxiv.org/pdf/1910.07467">RMSNorm paper</a>.</li>
</ul>
</section>
<section id="post-pre-normalization" class="level1">
<h1>2. Post / Pre Normalization</h1>
<p>The original Transformer (from the <em>“Attention Is All You Need”</em> paper) placed normalization layers <strong>after</strong> the attention and feedforward modules—this is known as <strong>Post-Norm</strong>.</p>
<p>GPT and most later LLMs use <strong>Pre-Norm</strong>, placing normalization layers <strong>before</strong> these modules.</p>
<p>In <a href="https://arxiv.org/abs/2002.04745">2020, Xiong et al.</a> showed that Pre-Norm leads to more stable gradients at initialization and can perform well without careful learning rate warm-up—unlike Post-Norm.</p>
<p>From an implementation standpoint, the change is minor: just swap the order of operations.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> We’re already using Pre-Norm here, so no code changes are needed.</p>
</blockquote>
<div id="cell-15" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Post Norm</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlockPostNorm(Block):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha_norm(<span class="va">self</span>.mha(x))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp_norm(<span class="va">self</span>.nlp(x))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pre Norm</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlockPostNorm(Block):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha(<span class="va">self</span>.mha_norm(x))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.nlp_norm(x))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="swiglu" class="level1">
<h1>3. SwiGLU</h1>
<p>Again, this is a small change to the FFN in the transformer block. Instead of a two-layer feedforward network with GeLU activation, we use SwiGLU.</p>
<p>It’s a short and easy-to-read paper: <a href="https://arxiv.org/pdf/2002.05202">GLU Variants Improve Transformer</a></p>
<div id="cell-18" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedFFN(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w1 <span class="op">=</span> nn.Linear(emb_dim, <span class="dv">4</span> <span class="op">*</span> <span class="bu">int</span>(<span class="dv">2</span> <span class="op">/</span> <span class="dv">3</span> <span class="op">*</span> emb_dim), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w3 <span class="op">=</span> nn.Linear(emb_dim, <span class="dv">4</span> <span class="op">*</span> <span class="bu">int</span>(<span class="dv">2</span> <span class="op">/</span> <span class="dv">3</span> <span class="op">*</span> emb_dim), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w2 <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> <span class="bu">int</span>(<span class="dv">2</span> <span class="op">/</span> <span class="dv">3</span> <span class="op">*</span> emb_dim), emb_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.silu_act <span class="op">=</span> nn.SiLU()</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.silu_act(<span class="va">self</span>.w1(x)) <span class="op">*</span> <span class="va">self</span>.w3(x)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.w2(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Here’s an excerpt from the paper explaining the rationale behind the <code>2/3</code> scaling factor. However, you can choose to ignore it—it was used mainly for apples-to-apples comparison in the paper.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="data:image/png;base64,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" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>At this point, the updated Transformer block (aka <code>Block</code> class) looks like this:</p>
<div id="cell-22" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, heads, context):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultiheadAttention(emb_dim, heads, context)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> GatedFFN(emb_dim)  <span class="co"># &lt;--- update</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha_norm <span class="op">=</span> nn.RMSNorm(emb_dim)  <span class="co"># &lt;--- update</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp_norm <span class="op">=</span> nn.RMSNorm(emb_dim)  <span class="co"># &lt;--- update</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha(<span class="va">self</span>.mha_norm(x))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.mlp_norm(x))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="rope" class="level1">
<h1>4. RoPE</h1>
<p>In the basic implementation, we use absolute position embeddings, as introduced in the original paper. However, things have evolved, and the current norm is to use relative position embeddings. Among various approaches, RoPE has become the dominant choice.</p>
<p>Incorporating RoPE requires more than just a few line changes. It might seem overwhelming, but it’s quite simple once you understand it. We essentially apply a transformation to our <code>Q</code> and <code>K</code> vectors before the attention operation.</p>
<p>Let’s first define the transformation.</p>
<div id="cell-25" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_freqs(dim, context_length, base<span class="op">=</span><span class="dv">10_000</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> dim <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>, <span class="st">"Embedding dimension should be even"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    inv_freq <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        base <span class="op">**</span> (torch.arange(<span class="dv">0</span>, dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">/</span> dim)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># shape: (1, dim//2)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    pos_ids <span class="op">=</span> torch.arange(context_length)  <span class="co"># shape: (context_len)</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    thetas <span class="op">=</span> pos_ids.unsqueeze(<span class="dv">1</span>) <span class="op">*</span> inv_freq  <span class="co"># shape: (context_len, dim//2)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    thetas <span class="op">=</span> torch.cat([thetas, thetas], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># shape: (context_len, dim)</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> thetas.cos(), thetas.sin()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rope(x, cos, sin):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    batch_size, heads, seq_len, emb_dim <span class="op">=</span> x.shape</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    x1, x2 <span class="op">=</span> x[..., : emb_dim <span class="op">//</span> <span class="dv">2</span>], x[..., emb_dim <span class="op">//</span> <span class="dv">2</span> :]</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    rotated <span class="op">=</span> torch.cat([<span class="op">-</span>x2, x1], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    cos, sin <span class="op">=</span> cos[:seq_len, :], sin[:seq_len, :]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    x_rotated <span class="op">=</span> (x <span class="op">*</span> cos) <span class="op">+</span> (rotated <span class="op">*</span> sin)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_rotated.to(dtype<span class="op">=</span>x.dtype)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>If the above code looks cryptic, please refer to my detailed notebook on RoPE. It implements RoPE in gradual steps to build better intuition, includes visuals, and compares different implementations.</p>
<p><strong>Note:</strong> Here, we follow the HuggingFace implementation of RoPE, as I plan to load the model weights from HF.</p>
<p>An important point about RoPE: unlike positional embeddings added once at the beginning, RoPE is applied in every transformer block.</p>
<div id="cell-27" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiheadAttention(nn.Module):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, heads, context):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> emb_dim <span class="op">%</span> heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"`emb_dim` should be a multiple of `heads`"</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> heads</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> emb_dim <span class="op">//</span> heads</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(emb_dim, <span class="dv">3</span> <span class="op">*</span> emb_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(emb_dim, emb_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RoPE and Casual Mask</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        cos, sin <span class="op">=</span> compute_freqs(<span class="va">self</span>.head_dim, context)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"cos"</span>, cos)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"sin"</span>, sin)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mask"</span>, torch.triu(torch.ones(context, context), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        batch, seq_len, emb_dim <span class="op">=</span> x.shape</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv_proj(x)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> qkv.view(batch, seq_len, <span class="dv">3</span>, <span class="va">self</span>.heads, <span class="va">self</span>.head_dim)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch, heads, seq_len, dim)</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        q, k <span class="op">=</span> apply_rope(q, <span class="va">self</span>.cos, <span class="va">self</span>.sin), apply_rope(k, <span class="va">self</span>.cos, <span class="va">self</span>.sin)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> (q <span class="op">@</span> k.mT) <span class="op">/</span> (<span class="va">self</span>.head_dim<span class="op">**</span><span class="fl">0.5</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> <span class="va">self</span>.mask[:seq_len, :seq_len]</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.masked_fill(mask, <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        attn_out <span class="op">=</span> torch.softmax(attn, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">@</span> v</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        attn_out <span class="op">=</span> attn_out.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        attn_out <span class="op">=</span> attn_out.reshape(batch, seq_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out_proj(attn_out)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, heads, context):</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultiheadAttention(emb_dim, heads, context)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> GatedFFN(emb_dim)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha_norm <span class="op">=</span> nn.RMSNorm(emb_dim)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp_norm <span class="op">=</span> nn.RMSNorm(emb_dim)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha(<span class="va">self</span>.mha_norm(x))</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.mlp_norm(x))</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.pos_embedding = nn.Embedding(config.context, config.emb_dim) # &lt;--- update</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embedding <span class="op">=</span> nn.Embedding(config.vocab, config.emb_dim)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>[</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>                Block(config.emb_dim, config.heads, config.context)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.layers)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(config.emb_dim, config.vocab, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.RMSNorm(config.emb_dim)  <span class="co"># &lt;--- update</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>        batch, seq_len <span class="op">=</span> x.shape</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pos = torch.arange(seq_len, device=x.device)         # &lt;--- update</span></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tok_embedding(x)  <span class="co"># + self.pos_embedding(pos) # &lt;--- update</span></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.decoder(x)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output(<span class="va">self</span>.norm(x))</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> device(<span class="va">self</span>):</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">next</span>(<span class="va">self</span>.parameters()).device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-28" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT(ModelConfig)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>count_parameters(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We have about 0.85 million fewer parameters, mainly by removing <code>pos_embedding</code>. We also eliminated bias in <code>GatedFFN</code>, and unlike <code>LayerNorm</code>, <code>RMSNorm</code> has no bias.</p>
<p>With these changes, let’s train the model and see if we get better scores and, consequently, better generations.</p>
</section>
<section id="training" class="level1">
<h1>Training</h1>
<div id="cell-31" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"wikitext"</span>, <span class="st">"wikitext-2-raw-v1"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(dataset[<span class="st">"test"</span>][<span class="st">"text"</span>])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(dataset[<span class="st">"train"</span>][<span class="st">"text"</span>])</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>val_tokens <span class="op">=</span> tokenizer.encode(val_ds)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>train_tokens <span class="op">=</span> tokenizer.encode(train_ds)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(val_tokens), <span class="bu">len</span>(train_tokens)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-32" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WikiTextDataset(Dataset):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokens, max_len):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokens <span class="op">=</span> tokens</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> idx <span class="op">*</span> <span class="va">self</span>.max_len</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tokens[idx : idx <span class="op">+</span> <span class="va">self</span>.max_len]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.tokens[idx <span class="op">+</span> <span class="dv">1</span> : idx <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> <span class="va">self</span>.max_len]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(x) <span class="op">&lt;</span> <span class="va">self</span>.max_len:</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x <span class="op">+</span> [tokenizer.eot_token] <span class="op">*</span> (<span class="va">self</span>.max_len <span class="op">-</span> <span class="bu">len</span>(x))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(y) <span class="op">&lt;</span> <span class="va">self</span>.max_len:</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y <span class="op">+</span> [tokenizer.eot_token] <span class="op">*</span> (<span class="va">self</span>.max_len <span class="op">-</span> <span class="bu">len</span>(y))</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (torch.tensor(x), torch.tensor(y))</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.ceil(<span class="bu">len</span>(<span class="va">self</span>.tokens) <span class="op">/</span> <span class="va">self</span>.max_len)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> WikiTextDataset(val_tokens, ModelConfig.context)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> WikiTextDataset(train_tokens, ModelConfig.context)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>val_dl <span class="op">=</span> DataLoader(val_ds, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(train_ds, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-33" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(model, tokenizer, prefix, max_new_tokens<span class="op">=</span><span class="dv">10</span>, temp<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="op">=</span> torch.tensor(tokenizer.encode(prefix), device<span class="op">=</span>device).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(token_ids)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> torch.softmax(logits <span class="op">/</span> temp, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># &lt;-- update: scale using temp</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        next_idx <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        prefix <span class="op">+=</span> tokenizer.decode([next_idx])</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        token_ids <span class="op">=</span> torch.cat((token_ids, next_idx), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prefix</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, dl):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> dl:</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(x)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> F.cross_entropy(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten()).cpu().item()</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss <span class="op">/</span> <span class="bu">len</span>(dl)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>evaluate(model, val_dl)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-34" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>prefix <span class="op">=</span> <span class="st">"Once upon a time"</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>log_freq <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(pbar <span class="op">:=</span> tqdm(train_dl, desc<span class="op">=</span><span class="st">"Training"</span>)):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> log_freq <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">=</span> evaluate(model, val_dl)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            losses.append(val_loss)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix_str(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">] Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), <span class="st">"model.pth"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(generate(model, tokenizer, prefix))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(x)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten())</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-35" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> torch.load(<span class="st">"model.pth"</span>, map_location<span class="op">=</span>device, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(state_dict)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-36" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Once upon a time"</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Internet is an"</span>))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"AI will"</span>))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"The meaning of life is"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="gradient-accumulation" class="level1">
<h1>Gradient Accumulation</h1>
<p>The generations are still not very good. Generally, larger batches help when training LLMs, but we’re limited by memory here.</p>
<p>Gradient Accumulation is a simple way to effectively increase the batch size.</p>
<div id="cell-39" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT(ModelConfig)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-40" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>prefix <span class="op">=</span> <span class="st">"Once upon a time"</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>log_freq <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>accumulate <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(pbar <span class="op">:=</span> tqdm(train_dl, desc<span class="op">=</span><span class="st">"Training"</span>)):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> log_freq <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">=</span> evaluate(model, val_dl)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            losses.append(val_loss)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix_str(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">] Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), <span class="st">"model.pth"</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(generate(model, tokenizer, prefix))</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(x)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten())</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> accumulate  <span class="co"># &lt;--- Update</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulate <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># &lt;--- Update</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-41" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> torch.load(<span class="st">"model.pth"</span>, map_location<span class="op">=</span>device, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(state_dict)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-42" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Once upon a time"</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Internet is an"</span>))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"AI will"</span>))</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"The meaning of life is"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="automatic-mixed-precision-amp" class="level1">
<h1>Automatic Mixed Precision (AMP)</h1>
<p>AMP is another technique to speed up training by using mixed precision instead of float32. It doesn’t reduce memory much but can speed up training. See the <a href="https://arxiv.org/pdf/1710.03740">paper</a>.</p>
<p>In PyTorch, it’s easy to implement: wrap the forward pass in <code>torch.autocast()</code>. For the backward pass, scale the loss with <code>scaler.scale(loss).backward()</code> and use <code>scaler.step(optimizer)</code> instead of <code>optimizer.step()</code>.</p>
<div id="cell-45" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT(ModelConfig)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-46" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, dl):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> dl:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span>device):  <span class="co"># &lt;--- Update</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(x)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> F.cross_entropy(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten()).cpu().item()</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss <span class="op">/</span> <span class="bu">len</span>(dl)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-47" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>prefix <span class="op">=</span> <span class="st">"Once upon a time"</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>log_freq <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>accumulate <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> torch.GradScaler()</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(pbar <span class="op">:=</span> tqdm(train_dl, desc<span class="op">=</span><span class="st">"Training"</span>)):</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> log_freq <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">=</span> evaluate(model, val_dl)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>            losses.append(val_loss)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix_str(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">] Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), <span class="st">"model.pth"</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(generate(model, tokenizer, prefix))</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span>device):  <span class="co"># &lt;--- Update</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(x)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> F.cross_entropy(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten())</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> accumulate</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>        scaler.scale(loss).backward()  <span class="co"># &lt;--- Update</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulate <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>            scaler.step(optimizer)  <span class="co"># &lt;--- Update</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>            scaler.update()  <span class="co"># &lt;--- Update</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> torch.load(<span class="st">"model.pth"</span>, map_location<span class="op">=</span>device, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(state_dict)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-49" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Once upon a time"</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"Internet is an"</span>))</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"AI will"</span>))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">15</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate(model, tokenizer, <span class="st">"The meaning of life is"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Another improvement we could add is a learning rate scheduler, but I’ll leave that for now. It’s also time to add WandB logging to track our training progress.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ankur-singh\.github\.io\/UnderstandingLLMs\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>