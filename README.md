# UnderstandingLLMs

A hands-on collection of Jupyter notebooks exploring key concepts, architectures, and optimizations in Large Language Models (LLMs).

---

## Notebooks

### 1. LLM from Scratch - Part 1
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/nbs/LLM_from_Scratch_1.ipynb)

- Build a basic LLM using PyTorch.
- Covers data preprocessing, model architecture, training loop, and inference.

---

### 2. LLM from Scratch - Part 2
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/nbs/LLM_from_Scratch_2.ipynb)

- Enhances the previous notebook with modern techniques:
    - RMSNorm
    - Gated (SwiGLU) FFN
    - Rotary Positional Embeddings (RoPE)
- Includes gradient accumulation and mixed precision training.

---

### 3. Rotary Positional Embeddings (RoPE)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/nbs/Rotary_Position_Embedding.ipynb)

- Deep dive into RoPE in PyTorch.
- Compares different RoPE implementations.

---

