---
title: "Understanding LLMs"
page-layout: full
title-block-banner: true
---

A hands-on collection of Jupyter notebooks exploring key concepts, architectures, and optimizations in Large Language Models (LLMs).


### Notebooks

1. **LLM from Scratch - Part 1**
[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/posts/LLM_from_Scratch_1.ipynb)


    - Build a basic LLM using PyTorch.
    - Covers data preprocessing, model architecture, training loop, and inference.

2. **LLM from Scratch - Part 2**
[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/posts/LLM_from_Scratch_2.ipynb)

    - Enhances the previous notebook with modern techniques:
        - RMSNorm
        - Gated (SwiGLU) FFN
        - Rotary Positional Embeddings (RoPE)
    - Includes gradient accumulation and mixed precision training.

3. **Rotary Positional Embeddings (RoPE)**
[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ankur-singh/UnderstandingLLMs/blob/main/posts/Rotary_Position_Embedding.ipynb)

    - Deep dive into RoPE in PyTorch.
    - Compares different RoPE implementations.


